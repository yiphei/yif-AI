{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_1.txt'\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "# seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "# random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# print(seed)\n",
    "# random.seed(1320387042447901345)\n",
    "# torch.manual_seed(1320387042447901345)\n",
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "for i in range(6):\n",
    "    shuffled_idx = torch.randperm(train_x.shape[0])\n",
    "    shuffled_x = train_x[shuffled_idx]\n",
    "    shuffled_y = train_y[shuffled_idx]\n",
    "    for x,y in zip(shuffled_x, shuffled_y):\n",
    "        out_1 = tm.forward(x.unsqueeze(0))\n",
    "        tm.update(y.unsqueeze(0))\n",
    "        out_2 = tm.forward(x.unsqueeze(0))\n",
    "        assert torch.equal(y.unsqueeze(0),out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0, 37, 37, 37,  0],\n",
       "         [ 0, 37, 37, 32,  0,  0],\n",
       "         [ 0,  0,  0, 16,  0, 16],\n",
       "         [37,  0,  0,  0, 32,  0],\n",
       "         [ 0,  0,  0,  0,  0, 22]]),\n",
       " tensor([[0, 0, 0, 0, 0]]),\n",
       " tensor([[31, 31, 15,  0,  0]]))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.l1.W,tm.l2.W[:, :tm.l2.W.shape[1]//2],tm.l2.W[:, tm.l2.W.shape[1]//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tm.forward(train_x)\n",
    "assert torch.equal(train_y, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_count = 0\n",
    "for _ in range(1000):\n",
    "    tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "    for i in range(6):\n",
    "        shuffled_idx = torch.randperm(train_x.shape[0])\n",
    "        shuffled_x = train_x[shuffled_idx]\n",
    "        shuffled_y = train_y[shuffled_idx]\n",
    "\n",
    "        for j, (x, y) in enumerate(zip(shuffled_x, shuffled_y)):\n",
    "            out_1 = tm.forward(x.unsqueeze(0))\n",
    "            tm.update(y.unsqueeze(0))\n",
    "            out_2 = tm.forward(x.unsqueeze(0))\n",
    "            assert torch.equal(y.unsqueeze(0), out_2)\n",
    "\n",
    "    out = tm.forward(train_x)\n",
    "    if not torch.equal(train_y, out):\n",
    "        failed_count += 1\n",
    "\n",
    "failed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "out_1 = tm.forward(train_x)\n",
    "out_1, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 1, 0, 0],\n",
       "         [0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]]),\n",
       " tensor([[1, 0, 0, 0, 1],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 1],\n",
       "         [1, 0, 1, 1, 0],\n",
       "         [1, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 1],\n",
       "         [0, 1, 1, 0, 0],\n",
       "         [1, 0, 1, 1, 0]]),\n",
       " tensor([[1, 1, 1, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 1, 0]]))"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.randint(0,2, tm.l1.out.size())\n",
    "tm.l1.out, Y, tm.l1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[], [0], [], [], [4], []],\n",
       " [[], [], [], [], [], []],\n",
       " [[], [0], [], [], [4], []],\n",
       " [[], [], [], [], [], []],\n",
       " [[], [], [], [], [], []],\n",
       " [[], [0], [], [], [4], []],\n",
       " [[], [], [], [], [], []],\n",
       " [[], [], [], [], [], []]]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_deps = []\n",
    "for i, single_x in enumerate(tm.l1.full_X):\n",
    "    flip_dep_row = [[] for _ in range(tm.l1.in_dim * 2)]\n",
    "    single_Y = Y[i]\n",
    "    if not torch.equal(single_Y, tm.l1.out[i]):\n",
    "        one_Y_idxs = torch.nonzero(single_Y == 1).squeeze(1)\n",
    "        W_halves = torch.split(tm.l1.W, tm.l1.in_dim, dim=1)\n",
    "        pos_W = W_halves[0]\n",
    "        neg_W = W_halves[1]\n",
    "        for pos_one_Y_idx in one_Y_idxs:\n",
    "            w_1 = pos_W[pos_one_Y_idx]\n",
    "            for neg_one_Y_idx in one_Y_idxs:\n",
    "                w_2 = neg_W[neg_one_Y_idx]\n",
    "                deps = ((w_1 == w_2) & (w_1 == 1))\n",
    "                if deps.any():\n",
    "                    dep_idxs = deps.nonzero(as_tuple=True)[0]\n",
    "                    for idx in dep_idxs:\n",
    "                        if pos_one_Y_idx.item() not in flip_dep_row[idx]:\n",
    "                            flip_dep_row[idx].append(pos_one_Y_idx.item())\n",
    "                        if neg_one_Y_idx.item() not in flip_dep_row[idx + tm.l1.in_dim]:\n",
    "                            flip_dep_row[idx + tm.l1.in_dim].append(neg_one_Y_idx.item())\n",
    "    flip_deps.append(flip_dep_row)\n",
    "flip_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 1, 1],\n",
       "        [1, 0, 1, 0, 1, 0],\n",
       "        [1, 1, 0, 0, 0, 1],\n",
       "        [1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 0, 1, 0, 1],\n",
       "        [0, 0, 1, 1, 1, 0],\n",
       "        [0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.l1.full_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[], [[], [], [], [], [], []]],\n",
       " [[], [[], [], [], [], [], []]],\n",
       " [[], [[], [], [], [], [], []]],\n",
       " [[1, 2], [[], [2, 5], [1, 6], [], [], []]],\n",
       " [[5], [[], [], [], [], [], [4]]]]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_deps = []\n",
    "for i, single_W in enumerate(tm.l1.W):\n",
    "    single_Y = Y[:, i]\n",
    "    single_out = tm.l1.out[:, i]\n",
    "    one_Y_idxs = torch.nonzero(single_Y == 1).squeeze(1)\n",
    "    x_prod = tm.l1.full_X[one_Y_idxs].prod(dim=0)    \n",
    "    must_be_one_idxs = torch.nonzero(x_prod == 1).squeeze(1)\n",
    "    zero_Y_idxs = torch.nonzero(single_Y == 0).squeeze(1)\n",
    "\n",
    "    must_be_flipped_row = [[] for _ in range(tm.l1.in_dim * 2)]\n",
    "    if len(must_be_one_idxs) > 0:\n",
    "        x_stuff = tm.l1.full_X[zero_Y_idxs][:, must_be_one_idxs] == 1\n",
    "        for row in range(x_stuff.shape[0]):\n",
    "            for col in range(x_stuff.shape[1]):\n",
    "                if x_stuff[row, col] and zero_Y_idxs[row].item() not in must_be_flipped_row[must_be_one_idxs[col]]:\n",
    "                    must_be_flipped_row[must_be_one_idxs[col]].append(zero_Y_idxs[row].item())\n",
    "\n",
    "    W_deps.append([must_be_one_idxs.tolist(), must_be_flipped_row])\n",
    "\n",
    "W_deps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
