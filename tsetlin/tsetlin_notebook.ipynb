{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "#TODO: need to add a constant random seed\n",
    "\n",
    "class TsetlinBase:\n",
    "    def conjunctin_mul(self, X, W):\n",
    "        matrix_X = X.repeat(W.shape[0], 1)\n",
    "        mask = W > 0 # TODO: prob need to compare and choose the clause with the highest weight\n",
    "        masked_X = torch.where(mask, matrix_X, torch.tensor(1))\n",
    "        return torch.prod(masked_X, dim=1, keepdim=True).view(1,-1)\n",
    "\n",
    "class TsetlinLayer(TsetlinBase):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        W_pos = torch.randint(0, 2, (out_dim, in_dim,))\n",
    "        W_neg = torch.randint(0, 2, (out_dim, in_dim,))\n",
    "        W_neg[W_pos == 1] = 0\n",
    "        self.W = torch.cat((W_pos, W_neg), dim=1)\n",
    "        zero_row_indices = (self.W.sum(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "        col_idx = torch.randint(0, in_dim * 2, (zero_row_indices.shape[0],))\n",
    "        self.W[zero_row_indices, col_idx] = 1\n",
    "\n",
    "        self.out = None\n",
    "        self.full_X = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_neg = 1 - X\n",
    "        self.full_X = torch.cat((X, X_neg), dim=0)\n",
    "        out = self.conjunctin_mul(self.full_X.unsqueeze(0), self.W)\n",
    "        self.out = out.squeeze(0)\n",
    "        return self.out\n",
    "    \n",
    "    def helper(self, updated_X,update_index, update_W, can_flip_value, can_remove, can_add_value):\n",
    "        # TODO: the random choice needs to be dynamic, otherwise if it is a very deep layer, it will be very hard to flip values in the earlier layers\n",
    "        flip_value = random.choice([True, False]) and can_flip_value\n",
    "        negation_index = (update_index + self.in_dim) % (self.in_dim * 2)\n",
    "        if flip_value:\n",
    "            updated_X[update_index] = 1 - updated_X[update_index]\n",
    "            updated_X[negation_index] = 1 - updated_X[negation_index]\n",
    "\n",
    "            #TODO: should i set the weight back to 0, as to descrease the confidence of the new flipped clause?\n",
    "            update_W[update_index] = 1\n",
    "            update_W[negation_index] = 0\n",
    "        else:\n",
    "            addable_indices = [ i for i, (w, v) in enumerate(zip(update_W, updated_X)) if w == 0 and v == 0 and update_W[(i + self.in_dim) % (self.in_dim * 2)] == 0 ] if can_add_value else []\n",
    "\n",
    "            add = random.choice([True, False]) and len(addable_indices) > 0\n",
    "            remove = random.choice([True, False]) and can_remove\n",
    "            if remove:\n",
    "                update_W[update_index] = 0\n",
    "            elif add:\n",
    "                add_index = random.choice(addable_indices)\n",
    "                update_W[add_index] = 1\n",
    "            else:\n",
    "                update_W[update_index] = 0\n",
    "                update_W[negation_index] = 1\n",
    "\n",
    "    def update(self, Y, is_first_layer = False):\n",
    "        can_flip_value = not (is_first_layer or torch.equal(Y, self.out))\n",
    "        if can_flip_value:\n",
    "            one_Y_indexes = torch.nonzero(Y == 1).squeeze(1)\n",
    "            W_halves = torch.split(self.W[one_Y_indexes], self.in_dim, dim=1)\n",
    "            pos_W = W_halves[0]\n",
    "            neg_W = W_halves[1]\n",
    "            for w_1 in pos_W:\n",
    "                indices = torch.nonzero(w_1 == 1).squeeze(1)\n",
    "                if any((w_1[indices] == w_2[indices]).any() for w_2 in neg_W):\n",
    "                    can_flip_value = False\n",
    "                    break\n",
    "\n",
    "        updated_X = torch.clone(self.full_X)\n",
    "        if torch.equal(Y, self.out):\n",
    "            # TODO: should this be done at every prior layer or should it stop at this layer?\n",
    "            self.W[self.W > 0] += 1\n",
    "        else:\n",
    "            one_Y_indexes = torch.nonzero((Y == 1) & (Y != self.out)).squeeze(1)\n",
    "            for row_index in one_Y_indexes:\n",
    "                update_indices = [ i for i, (w, v) in enumerate(zip(self.W[row_index], updated_X)) if w > 0 and v == 0]\n",
    "                for update_index in update_indices:\n",
    "                    self.helper(updated_X, update_index, self.W[row_index.item()], can_flip_value, True, False)\n",
    "\n",
    "            updated_out = self.conjunctin_mul(updated_X.unsqueeze(0), self.W).squeeze(0) if not torch.equal(updated_X, self.full_X) else self.out\n",
    "            zero_Y_indexes = torch.nonzero((Y == 0) & (Y != updated_out)).squeeze(1)\n",
    "\n",
    "            for row_index in zero_Y_indexes:\n",
    "                target_indexes = []\n",
    "                min_confidence = 0\n",
    "                for j in range(self.in_dim * 2):\n",
    "                    W_value = self.W[row_index.item()][j]\n",
    "                    X_value = updated_X[j]\n",
    "                    if W_value > 0 and X_value == 1:\n",
    "                        if W_value < min_confidence or len(target_indexes) == 0:\n",
    "                            target_indexes = [j]\n",
    "                            min_confidence = W_value\n",
    "                        elif W_value == min_confidence:\n",
    "                            target_indexes.append(j)\n",
    "\n",
    "                # TODO: there's a bug here when the layer is l1. Fix it\n",
    "                update_index = random.choice(target_indexes)\n",
    "                self.helper(updated_X,update_index, self.W[row_index.item()], can_flip_value, False, True)\n",
    "        return updated_X[:self.in_dim]\n",
    "\n",
    "class TsetlinMachine:\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        self.l1 = TsetlinLayer(in_dim, 10)\n",
    "        self.l2 = TsetlinLayer(10, 1)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.l1.forward(X)\n",
    "        X = self.l2.forward(X)\n",
    "        self.out = X.squeeze(0)\n",
    "        return self.out\n",
    "    \n",
    "    def update(self, y):\n",
    "        y = torch.tensor([y])\n",
    "        updated_X = self.l2.update(y)\n",
    "        self.l1.update(updated_X, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10060444818134316592\n",
      "10756460437144971782\n",
      "2661695040213583467\n",
      "4495657811875794371\n",
      "16549216921405580979\n",
      "9702253675113545912\n",
      "14778163255289327925\n",
      "382768689850443838\n",
      "2346777943487131851\n",
      "11021411444210376573\n",
      "7072922956099336456\n",
      "14784230048355905464\n",
      "5193141934604491883\n",
      "14904802531311000661\n",
      "3225440795958774044\n",
      "6134267903656654277\n",
      "9084611591844557592\n",
      "11155869803106907967\n",
      "625888439734751270\n",
      "1685357587340371835\n",
      "2670432517292671888\n",
      "15283054345136119450\n",
      "4016829170655325605\n",
      "6714579498780288074\n",
      "6397947761062513705\n",
      "17534066357051868017\n",
      "6062408116716767487\n",
      "15172348566445126762\n",
      "12048722645582097827\n",
      "14112599496965752905\n",
      "8061481361300197117\n",
      "7434664880808499796\n",
      "13952978879343121585\n",
      "16405182923127298481\n",
      "3658152974813704363\n",
      "8039260064572111773\n",
      "3619944579145170560\n",
      "3904310115586672831\n",
      "10323991585731588932\n",
      "2393769122730818508\n",
      "13174079050614380555\n",
      "16214899962562345556\n",
      "5773759788029078502\n",
      "7293206688044867386\n",
      "8549386146724611766\n",
      "7374909536016257417\n",
      "13176241616961413415\n",
      "2160671656021251907\n",
      "4350914087568905019\n",
      "14034768577743015261\n",
      "4570189549928837340\n",
      "2681592403894308617\n",
      "8364220078371376825\n",
      "5217555316024424488\n",
      "14420229783984096621\n",
      "11356817608741861335\n",
      "10957527585637396530\n",
      "9984682710867236947\n",
      "12895377778533162037\n",
      "2584403470695217000\n",
      "717597510384616978\n",
      "15678397139238351933\n",
      "7095674362726422143\n",
      "5029602331395133337\n",
      "18436997904150983524\n",
      "7778141403188051419\n",
      "4693186729870171920\n",
      "11200712460850686605\n",
      "6919485433967970263\n",
      "9153681762972441901\n",
      "8234248383817775422\n",
      "9528732476907442233\n",
      "7589098064430055811\n",
      "8074631753620372775\n",
      "12394754167170311391\n",
      "9577710164409654051\n",
      "8919093569035887778\n",
      "1032358960674869339\n",
      "12385081113961490881\n",
      "1191867901231295487\n",
      "7894838141901035976\n",
      "5369093715888292205\n",
      "1873852859011676632\n",
      "4596916303659200561\n",
      "4233518693249837728\n",
      "5929678143922049377\n",
      "13445364740101310621\n",
      "5189793496548698022\n",
      "454725312123663913\n",
      "2835983574246154222\n",
      "17626540706935739622\n",
      "16312533889212003875\n",
      "18201384801396330904\n",
      "8947770908814133434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for _ in range(20000):\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # random.seed(2278134817936846448)\n",
    "    # torch.manual_seed(2278134817936846448)\n",
    "\n",
    "    tm = TsetlinMachine(2)\n",
    "    out_1 = tm.forward(torch.tensor([0,1])).item()\n",
    "    if out_1 == 1:\n",
    "        print(seed)\n",
    "    actual = 0 if out_1 == 1 else 1\n",
    "    tm.update(actual)\n",
    "    out_2 = tm.forward(torch.tensor([0,1])).item()\n",
    "    assert actual == out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
