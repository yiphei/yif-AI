{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class TsetlinBase:\n",
    "    def conjunctin_mul(self, X, W):\n",
    "        matrix_X = X.repeat(W.shape[0], 1)\n",
    "        mask = W > 0 # TODO: prob need to compare and choose the clause with the highest weight\n",
    "        masked_X = torch.where(mask, matrix_X, torch.tensor(1))\n",
    "        return torch.prod(masked_X, dim=1, keepdim=True).view(1,-1)\n",
    "\n",
    "class TsetlinLayer(TsetlinBase):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        W_pos = torch.randint(0, 2, (out_dim, in_dim,))\n",
    "        W_neg = torch.randint(0, 2, (out_dim, in_dim,))\n",
    "        W_neg[W_pos == 1] = 0\n",
    "        self.W = torch.cat((W_pos, W_neg), dim=1)\n",
    "        self.out = None\n",
    "        self.full_X = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_neg = 1 - X\n",
    "        self.full_X = torch.cat((X, X_neg), dim=0)\n",
    "        out = self.conjunctin_mul(self.full_X.unsqueeze(0), self.W)\n",
    "        self.out = out.squeeze(0)\n",
    "        return self.out\n",
    "    \n",
    "    def helper(self, updated_X,update_index, update_W, can_flip_value, can_remove, can_add_value):\n",
    "        # TODO: the random choice needs to be dynamic, otherwise if it is a very deep layer, it will be very hard to flip values in the earlier layers\n",
    "        flip_value = random.choice([True, False]) and can_flip_value\n",
    "        negation_index = (update_index + self.in_dim) % self.in_dim\n",
    "        if flip_value:\n",
    "            updated_X[update_index] = 1 - updated_X[update_index]\n",
    "            updated_X[negation_index] = 1 - updated_X[negation_index]\n",
    "\n",
    "            #TODO: should i set the weight back to 0, as to descrease the confidence of the new flipped clause?\n",
    "            update_W[update_index] = 0\n",
    "            update_W[negation_index] = 1\n",
    "        else:\n",
    "            addable_indices = [ i for i, (w, v) in enumerate(zip(update_W, updated_X)) if w == 0 and v == 0 and update_W[(i + self.in_dim) % self.in_dim] == 0 ] if can_add_value else []\n",
    "\n",
    "            add = random.choice([True, False]) and len(addable_indices) > 0\n",
    "            remove = random.choice([True, False]) and can_remove\n",
    "            if remove:\n",
    "                update_W[update_index] = 0\n",
    "            elif add:\n",
    "                add_index = random.choice(addable_indices)\n",
    "                update_index[add_index] = 1\n",
    "            else:\n",
    "                update_W[update_index] = 0\n",
    "                update_W[negation_index] = 1\n",
    "\n",
    "    def update(self, Y, is_first_layer = False):\n",
    "        can_flip_value = not (is_first_layer or torch.equal(Y, self.out))\n",
    "        if can_flip_value:\n",
    "            one_Y_indexes = torch.nonzero(Y == 1).squeeze(0)\n",
    "            W_halves = torch.split(self.W[one_Y_indexes], self.in_dim, dim=1)\n",
    "            pos_W = W_halves[0]\n",
    "            neg_W = W_halves[1]\n",
    "            for w_1 in pos_W:\n",
    "                indices = torch.nonzero(w_1 == 1).squeeze(0)\n",
    "                if any((w_1[indices] == w_2[indices]).any() for w_2 in neg_W):\n",
    "                    can_flip_value = False\n",
    "                    break\n",
    "\n",
    "        updated_X = torch.clone(self.full_X)\n",
    "        if torch.equal(Y, self.out):\n",
    "            # TODO: should this be done at every prior layer or should it stop at this layer?\n",
    "            self.W[self.W > 0] += 1\n",
    "        else:\n",
    "            one_Y_indexes = torch.nonzero((Y == 1) | (Y != self.out)).squeeze(0)\n",
    "            update_Ws = self.W[one_Y_indexes]\n",
    "\n",
    "            for update_W in update_Ws:\n",
    "                update_indices = [ i for i, (w, v) in enumerate(zip(update_W, updated_X)) if w > 0 and v == 0]\n",
    "                for update_index in update_indices:\n",
    "                    self.helper(updated_X,update_index, update_W, can_flip_value, True, False)\n",
    "\n",
    "            updated_out = self.conjunctin_mul(updated_X.unsqueeze(0), self.W) if not torch.equal(updated_X, self.full_X) else self.out\n",
    "            zero_Y_indexes = torch.nonzero((Y == 0) | (Y != updated_out)).squeeze(0)\n",
    "            update_Ws = self.W[zero_Y_indexes]\n",
    "            for update_W in update_Ws:\n",
    "                target_indexes = []\n",
    "                min_confidence = 0\n",
    "                for j in range(self.in_dim * 2):\n",
    "                    W_value = update_W[j]\n",
    "                    X_value = updated_X[j]\n",
    "                    if W_value > 0 and X_value == 1:\n",
    "                        if W_value < min_confidence or len(target_indexes) == 0:\n",
    "                            target_indexes = [j]\n",
    "                            min_confidence = W_value\n",
    "                        else:\n",
    "                            target_indexes.append(j)\n",
    "\n",
    "                    update_index = random.choice(target_indexes)\n",
    "                    self.helper(updated_X,update_index, update_W, can_flip_value, False, True)\n",
    "        return updated_X\n",
    "\n",
    "class TsetlinMachine:\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        self.l1 = TsetlinLayer(in_dim, 10)\n",
    "        self.l2 = TsetlinLayer(10, 1)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.l1.forward(X)\n",
    "        X = self.l2.forward(X)\n",
    "        self.out = X.squeeze(0)\n",
    "        return self.out\n",
    "    \n",
    "    def update(self, y):\n",
    "        y = torch.tensor([y])\n",
    "        updated_X = self.l2.update(y)\n",
    "        self.l1.update(updated_X, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm = TsetlinMachine(2)\n",
    "tm.forward(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tm\u001b[39m.\u001b[39;49mupdate(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([y])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     updated_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml2\u001b[39m.\u001b[39;49mupdate(y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1\u001b[39m.\u001b[39mupdate(updated_X, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m can_flip_value:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     one_Y_indexes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero(Y \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     W_halves \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msplit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW[one_Y_indexes], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_dim, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     pos_W \u001b[39m=\u001b[39m W_halves[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/tsetlin_notebook.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     neg_W \u001b[39m=\u001b[39m W_halves[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/micrograd-yifei/lib/python3.10/site-packages/torch/functional.py:189\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    184\u001b[0m         split, (tensor,), tensor, split_size_or_sections, dim\u001b[39m=\u001b[39mdim)\n\u001b[1;32m    185\u001b[0m \u001b[39m# Overwriting reason:\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m# call here.\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msplit(split_size_or_sections, dim)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/envs/micrograd-yifei/lib/python3.10/site-packages/torch/_tensor.py:801\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(split_size, (\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39mSymInt)):\n\u001b[0;32m--> 801\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_VF\u001b[39m.\u001b[39;49msplit(\u001b[39mself\u001b[39;49m, split_size, dim)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_VF\u001b[39m.\u001b[39msplit_with_sizes(\u001b[39mself\u001b[39m, split_size, dim)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "tm.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
