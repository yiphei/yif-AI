{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
      "        [0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0],\n",
      "        [0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1],\n",
      "        [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0],\n",
      "        [0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
      "        [0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
      "        [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1],\n",
      "        [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1],\n",
      "        [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0],\n",
      "        [1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
      "        [1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1],\n",
      "        [1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1],\n",
      "        [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
      "        [1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
      "        [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]])\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 1, 0]])\n",
      "{0: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 44, 45, 46, 48, 49, 50, 52, 53, 54, 56, 57, 58, 60, 61, 62}, 1: {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 61, 62, 63}, 2: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 63}, 3: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 56, 57, 58, 59, 60, 62}, 4: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 21, 22, 23, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63}}\n",
      "{0: {35, 39, 43, 47, 51, 55, 59, 63}, 1: {8, 40, 12, 44, 60, 24, 56, 28}, 2: {58, 62}, 3: {63, 61, 53, 21, 23, 55, 29, 31}, 4: {16, 24, 20, 28}}\n",
      "deque([0, 1, 3, 4, 2])\n",
      "AAAA\n",
      "AAAA\n",
      "AAAA\n",
      "AAAA\n",
      "AAAA\n",
      "AAAA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39mprint\u001b[39m(W_row_to_one_Y_row_idxs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mprint\u001b[39m(q)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m new_X_row_idxs_per_W_col, is_solved \u001b[39m=\u001b[39m get_new_X_row_idxs_per_W_col(\u001b[39m0\u001b[39;49m, tm\u001b[39m.\u001b[39;49ml1\u001b[39m.\u001b[39;49min_dim, one_Y_row_state, q\u001b[39m.\u001b[39;49mpopleft(), q) \u001b[39m# X_row_idxs_per_W_col does not necessarily contain a slot for each col\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m# assert is_solved\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m# # new_X_row_idxs_per_W_col provides a valid update of W and full_X that satisfies the expected Y.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m \u001b[39m# _, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(set(range(len(new_X_row_idxs_per_W_col))), None, set(), None)\u001b[39;00m\n",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sub_diff) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         updated_one_Y_row_state[k] \u001b[39m=\u001b[39m sub_diff\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m sub_new_X_row_idxs_per_W_col, is_solved \u001b[39m=\u001b[39m get_new_X_row_idxs_per_W_col(depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy\u001b[39m.\u001b[39;49mdeepcopy(q))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mif\u001b[39;00m is_solved:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     new_X_row_idxs_per_W_col \u001b[39m=\u001b[39m sub_new_X_row_idxs_per_W_col\n",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sub_diff) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         updated_one_Y_row_state[k] \u001b[39m=\u001b[39m sub_diff\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m sub_new_X_row_idxs_per_W_col, is_solved \u001b[39m=\u001b[39m get_new_X_row_idxs_per_W_col(depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy\u001b[39m.\u001b[39;49mdeepcopy(q))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mif\u001b[39;00m is_solved:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     new_X_row_idxs_per_W_col \u001b[39m=\u001b[39m sub_new_X_row_idxs_per_W_col\n",
      "    \u001b[0;31m[... skipping similar frames: get_new_X_row_idxs_per_W_col at line 145 (3 times)]\u001b[0m\n",
      "\u001b[1;32m/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sub_diff) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         updated_one_Y_row_state[k] \u001b[39m=\u001b[39m sub_diff\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m sub_new_X_row_idxs_per_W_col, is_solved \u001b[39m=\u001b[39m get_new_X_row_idxs_per_W_col(depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy\u001b[39m.\u001b[39;49mdeepcopy(q))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mif\u001b[39;00m is_solved:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yifeiyan/micrograd-yifei/tsetlin/test_min_sum_notebook.ipynb#W0sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     new_X_row_idxs_per_W_col \u001b[39m=\u001b[39m sub_new_X_row_idxs_per_W_col\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/copy.py:288\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m    287\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m listiter:\n\u001b[0;32m--> 288\u001b[0m         item \u001b[39m=\u001b[39m deepcopy(item, memo)\n\u001b[1;32m    289\u001b[0m         y\u001b[39m.\u001b[39mappend(item)\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/copy.py:175\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n\u001b[1;32m    176\u001b[0m     memo[d] \u001b[39m=\u001b[39m y\n\u001b[1;32m    177\u001b[0m     _keep_alive(x, memo) \u001b[39m# Make sure x lives at least as long as d\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "TRACKING = {0:0}\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_2.txt'\n",
    "# SEED = 5779661865816281544\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "\n",
    "def generate_subsets_iterator(set_elements, subset_size):\n",
    "    return combinations(set_elements, subset_size)\n",
    "\n",
    "def generate_powerset_iterator(set_elements):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    return chain.from_iterable(combinations(set_elements,r) for r in range(len(set_elements), -1, -1))\n",
    "\n",
    "\n",
    "def combine_iterators(iterable_one, iterable_two):\n",
    "    for item in iterable_one:\n",
    "        yield item\n",
    "\n",
    "    for item in iterable_two:\n",
    "        set_item = set(item)\n",
    "        if set_item not in iterable_one:\n",
    "            yield set_item\n",
    "\n",
    "for _ in range(1):\n",
    "    SEED = None\n",
    "    if SEED:\n",
    "        random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "    else:\n",
    "        seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # print(seed)\n",
    "\n",
    "    tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "\n",
    "    tm.forward(train_x)\n",
    "    print(tm.l1.full_X)\n",
    "    print(tm.l1.W)\n",
    "    print(tm.l1.out)\n",
    "\n",
    "    W_row_to_zero_Y_row_idxs = {}\n",
    "    W_row_to_one_Y_row_idxs = {}\n",
    "    for i in range(tm.l1.W.shape[0]):\n",
    "        row_Y = tm.l1.out[:, i]\n",
    "        \n",
    "        zero_Y_idxs = torch.nonzero(row_Y == 0).squeeze(1).tolist()\n",
    "        if zero_Y_idxs:\n",
    "            W_row_to_zero_Y_row_idxs[i] = set(zero_Y_idxs)\n",
    "\n",
    "        one_Y_idxs = torch.nonzero(row_Y == 1).squeeze(1).tolist()\n",
    "        if one_Y_idxs:\n",
    "            W_row_to_one_Y_row_idxs[i] = set(one_Y_idxs)\n",
    "\n",
    "    W_rows_of_unique_one_Y_row_idxs = set() # this is constructed by assigning one_Y_row_idxs to W columns incrementally (from 0). This will later be changed\n",
    "    visited_one_Y_row_idxs = set()\n",
    "\n",
    "    W_col_to_new_X_row_idxs = {}\n",
    "    for W_row_idx, one_Y_row_idxs in W_row_to_one_Y_row_idxs.items():\n",
    "        tuple_value = tuple(one_Y_row_idxs)\n",
    "        if tuple_value not in visited_one_Y_row_idxs:\n",
    "            visited_one_Y_row_idxs.add(tuple_value)\n",
    "            W_rows_of_unique_one_Y_row_idxs.add(W_row_idx)\n",
    "\n",
    "    one_Y_row_state = {W_row: W_row_to_zero_Y_row_idxs.get(W_row, set())  for W_row in W_rows_of_unique_one_Y_row_idxs} # this tracks unresolved zero Y row idxs for each W row idx\n",
    "    sorted_one_Y_row_idxs = sorted(list(W_rows_of_unique_one_Y_row_idxs), key=lambda x: len(W_row_to_one_Y_row_idxs[x]), reverse=True) # a heuristical optimization to address the largest one_Y_row_idxs first\n",
    "    q = deque(sorted_one_Y_row_idxs)\n",
    "\n",
    "    def get_new_X_row_idxs_per_W_col(depth, max_depth, curr_one_Y_row_state, prev_W_row_idx, q):\n",
    "        # the output is of shape [({1,2,3},{4,5,6}), ({2,3},{4,5,1,6}), ...] where ({1,2,3},{4,5,6}) means\n",
    "        # that W[[1,2,3]][0] should be 1 and W[[4,5,6]][0] should be 0 and full_X[[1,2,3]][0] should be 1 \n",
    "        # and full_X[[4,5,6]][0] should be 0\n",
    "\n",
    "        if depth == max_depth or len(curr_one_Y_row_state) == 0:\n",
    "            return [], len(curr_one_Y_row_state) == 0\n",
    "\n",
    "        curr_W_row_idx = prev_W_row_idx\n",
    "        while curr_W_row_idx not in curr_one_Y_row_state and q:\n",
    "            curr_W_row_idx = q.popleft()\n",
    "\n",
    "        curr_one_Y_idxs = W_row_to_one_Y_row_idxs[curr_W_row_idx]\n",
    "        min_zero_Y_idxs_len = math.ceil(len(curr_one_Y_row_state[curr_W_row_idx]) / (max_depth - depth)) # a heuristical optimization to ensure that zero Y row idxs are steadily resolved\n",
    "\n",
    "        # heuristical optimization to align unresolved zero Y row idxs to unresolved one Y row idxs\n",
    "        ordered_min_zero_Y_subsets = []\n",
    "        remaining_q = list(q)\n",
    "        for W_row_idx in remaining_q:\n",
    "            one_Y_idxs = W_row_to_one_Y_row_idxs[W_row_idx]\n",
    "            if len(one_Y_idxs) == min_zero_Y_idxs_len and len(one_Y_idxs & curr_one_Y_idxs) == 0 and len(one_Y_idxs & curr_one_Y_row_state[curr_W_row_idx]) > 0:\n",
    "                ordered_min_zero_Y_subsets.append(one_Y_idxs)\n",
    "\n",
    "        for min_zero_Y_subset in combine_iterators(ordered_min_zero_Y_subsets, generate_subsets_iterator(curr_one_Y_row_state[curr_W_row_idx], min(min_zero_Y_idxs_len, len(curr_one_Y_row_state[curr_W_row_idx])))):\n",
    "            remaining_Y_idxs = set(range(tm.l1.full_X.shape[0])) - (min_zero_Y_subset | curr_one_Y_idxs)\n",
    "            print(\"AAAA\")\n",
    "            # same heuristical optimization as above with ordered_min_zero_Y_subsets\n",
    "            remaining_Y_subsets_ordered = []\n",
    "            for W_row_idx in remaining_q:\n",
    "                one_Y_idxs = W_row_to_one_Y_row_idxs[W_row_idx]\n",
    "                if one_Y_idxs.issubset(remaining_Y_idxs):\n",
    "                    remaining_Y_subsets_ordered.append(one_Y_idxs)\n",
    "\n",
    "            for remaining_Y_subset in combine_iterators(remaining_Y_subsets_ordered, generate_powerset_iterator(remaining_Y_idxs)):\n",
    "                complement_remaining_Y_subset = remaining_Y_idxs - remaining_Y_subset\n",
    "\n",
    "                first_left_W = curr_one_Y_idxs | complement_remaining_Y_subset\n",
    "                first_right_W = min_zero_Y_subset | remaining_Y_subset\n",
    "\n",
    "                second_left_W = curr_one_Y_idxs | remaining_Y_subset\n",
    "                second_right_W = min_zero_Y_subset | complement_remaining_Y_subset\n",
    "\n",
    "                for left_W, right_W in [(first_left_W, first_right_W), (second_left_W, second_right_W)]:\n",
    "                    updated_one_Y_row_state = {}\n",
    "                    for k,v in curr_one_Y_row_state.items():\n",
    "                        one_Y_idxs = W_row_to_one_Y_row_idxs[k]\n",
    "                        sub_diff = v\n",
    "\n",
    "                        if one_Y_idxs.issubset(left_W):\n",
    "                            sub_diff = v - right_W\n",
    "                        elif one_Y_idxs.issubset(right_W):\n",
    "                            sub_diff = v - left_W\n",
    "                        \n",
    "                        # implicit here is the removal of one_Y_idxs for which there is no unresolved zero Y row idxs left\n",
    "                        if len(sub_diff) > 0:\n",
    "                            updated_one_Y_row_state[k] = sub_diff\n",
    "\n",
    "                    sub_new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(depth+1, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy.deepcopy(q))\n",
    "                    if is_solved:\n",
    "                        new_X_row_idxs_per_W_col = sub_new_X_row_idxs_per_W_col\n",
    "                        new_X_row_idxs_per_W_col.append((left_W, right_W))\n",
    "                        return new_X_row_idxs_per_W_col, True\n",
    "                    \n",
    "        return [], False\n",
    "    \n",
    "    print(W_row_to_zero_Y_row_idxs)\n",
    "    print(W_row_to_one_Y_row_idxs)\n",
    "    print(q)\n",
    "    new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(0, tm.l1.in_dim, one_Y_row_state, q.popleft(), q) # X_row_idxs_per_W_col does not necessarily contain a slot for each col\n",
    "\n",
    "\n",
    "\n",
    "    # assert is_solved\n",
    "\n",
    "    # # new_X_row_idxs_per_W_col provides a valid update of W and full_X that satisfies the expected Y.\n",
    "    # # However, we assigned one_Y_row_idxs to W columns incrementally (from 0) for simplicity.\n",
    "    # # Below, we determine the best W column assignment based on W_confidence.\n",
    "\n",
    "    # W_row_idxs_per_col = defaultdict(lambda: [[], []]) # this represents all one_W_row_idxs and zero_W_row_idxs pairs\n",
    "    # for W_row_idx, one_Y_row_idxs in W_row_to_one_Y_row_idxs.items():\n",
    "    #     for W_col_idx, new_X_row_idxs in enumerate(new_X_row_idxs_per_W_col):\n",
    "    #         if one_Y_row_idxs.issubset(new_X_row_idxs[0]):\n",
    "    #             W_row_idxs_per_col[W_col_idx][0].append(W_row_idx)\n",
    "    #         elif one_Y_row_idxs.issubset(new_X_row_idxs[1]):\n",
    "    #             W_row_idxs_per_col[W_col_idx][1].append(W_row_idx)\n",
    "\n",
    "    \n",
    "    # W_confidence = torch.randint_like(tm.l1.W, 0, 1)\n",
    "\n",
    "    # # calculate the W_confidence sum of each one_W_row_idxs and zero_W_row_idxs pairs for all W columns\n",
    "    # W_row_idxs_sets_confidence_sum_per_col = []\n",
    "    # for W_row_idxs in W_row_idxs_per_col.keys():\n",
    "    #     sums = W_confidence[W_row_idxs_per_col[W_row_idxs][0]].sum(dim=0)\n",
    "    #     neg_sum = torch.roll(W_confidence[W_row_idxs_per_col[W_row_idxs][1]].sum(dim=0), shifts = -tm.l1.in_dim, dims=0)\n",
    "    #     sums += neg_sum\n",
    "    #     W_row_idxs_sets_confidence_sum_per_col.append(sums)\n",
    "        \n",
    "    # W_row_idxs_sets_confidence_sum_per_col = torch.stack(W_row_idxs_sets_confidence_sum_per_col)\n",
    "    # sorted_W_row_idxs_sets_confidence_sum_per_col = torch.sort(W_row_idxs_sets_confidence_sum_per_col, dim=1, descending=False) # sort by increasing sum\n",
    "    \n",
    "    # # Prune W columns\n",
    "    # opt_values = []\n",
    "    # opt_indices = []\n",
    "\n",
    "    # for sum_values, sum_indices in zip(sorted_W_row_idxs_sets_confidence_sum_per_col.values, sorted_W_row_idxs_sets_confidence_sum_per_col.indices):\n",
    "    #     opt_sums = []\n",
    "    #     opt_sum_idxs = []\n",
    "    #     visited_col_idxs = set()\n",
    "    #     for sum_value, idx in zip(sum_values, sum_indices):\n",
    "    #         idx_value = idx.item()\n",
    "    #         if idx_value % tm.l1.in_dim not in visited_col_idxs:\n",
    "    #             opt_sums.append(sum_value.item())\n",
    "    #             opt_sum_idxs.append(idx_value)\n",
    "    #             visited_col_idxs.add(idx_value % tm.l1.in_dim)\n",
    "\n",
    "    #         if len(visited_col_idxs) == tm.l1.in_dim:\n",
    "    #             break\n",
    "        \n",
    "    #     opt_values.append(opt_sums)\n",
    "    #     opt_indices.append(opt_sum_idxs)\n",
    "\n",
    "    # sorted_W_row_idxs_sets_confidence_sum_per_col_values = torch.tensor(opt_values)\n",
    "    # sorted_W_row_idxs_sets_confidence_sum_per_col_indices = torch.tensor(opt_indices) \n",
    "    \n",
    "    # # a heuristical optimization that sorts W columns by increasing offset sum across one_W_row_idxs and zero_W_row_idxs pairs\n",
    "    # offset_sorted_W_row_idxs_sets_confidence_sum_per_col = sorted_W_row_idxs_sets_confidence_sum_per_col_values - sorted_W_row_idxs_sets_confidence_sum_per_col_values[:, 0].unsqueeze(1) # normalize the sum by subtracting the smallest sum\n",
    "    # offset_W_row_idxs_sets_confidence_sum_to_cols_dict = defaultdict(set)\n",
    "    # for col_idx, offset_sums in enumerate(offset_sorted_W_row_idxs_sets_confidence_sum_per_col):\n",
    "    #     for offset_sum in offset_sums:\n",
    "    #         offset_W_row_idxs_sets_confidence_sum_to_cols_dict[offset_sum.item()].add(col_idx)\n",
    "    # sorted_W_row_idxs_sets_confidence_sum = sorted(offset_W_row_idxs_sets_confidence_sum_to_cols_dict.keys())\n",
    "    # W_row_idxs_set_sequencing = [offset_W_row_idxs_sets_confidence_sum_to_cols_dict[x] for x in sorted_W_row_idxs_sets_confidence_sum] # based on increasing offset W row idxs sets sum\n",
    "\n",
    "    # def get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(W_row_idxs_set_idxs, max_sorted_idx_per_W_row_idxs_set_idxs, used_W_col_idxs, max_sum):\n",
    "    #     # This is the core function that determines the best W column assignment based on W_confidence. Before was all preprocessing for a faster algorithm.\n",
    "    #     # The output shape is {0:1, 1:0, 2:5} where 0:1 means that one_W_row_idxs and zero_W_row_idxs pair indexed at 0 should be assigned to W column 1\n",
    "        \n",
    "    #     if len(W_row_idxs_set_idxs) == 1:\n",
    "    #         W_row_idxs_set_idx = list(W_row_idxs_set_idxs)[0]\n",
    "    #         max_sorted_idx = max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx] if max_sorted_idx_per_W_row_idxs_set_idxs is not None else tm.l1.in_dim - 1\n",
    "            \n",
    "    #         for sorted_idx in range(max_sorted_idx + 1):\n",
    "    #             col_idx = sorted_W_row_idxs_sets_confidence_sum_per_col_indices[W_row_idxs_set_idx, sorted_idx].item()\n",
    "    #             W_row_idxs_confidence_sum = sorted_W_row_idxs_sets_confidence_sum_per_col_values[W_row_idxs_set_idx, sorted_idx].item()\n",
    "\n",
    "    #             if max_sum is not None and W_row_idxs_confidence_sum >= max_sum:\n",
    "    #                 return None, None\n",
    "    #             if col_idx % tm.l1.in_dim not in used_W_col_idxs:\n",
    "    #                 return W_row_idxs_confidence_sum, {W_row_idxs_set_idx: sorted_idx}\n",
    "\n",
    "    #         return None, None\n",
    "\n",
    "    #     curr_max_sorted_idx_per_W_row_idxs_set_idxs = [-1] * len(new_X_row_idxs_per_W_col)\n",
    "    #     min_confidence_sum = max_sum\n",
    "    #     W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = None\n",
    "\n",
    "    #     for i in range(len(W_row_idxs_set_sequencing)):\n",
    "    #         W_row_idxs_set_idx = W_row_idxs_set_sequencing[i]\n",
    "    #         if W_row_idxs_set_idx not in W_row_idxs_set_idxs:\n",
    "    #             continue\n",
    "\n",
    "    #         curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx] += 1\n",
    "    #         if max_sorted_idx_per_W_row_idxs_set_idxs is not None and curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx] > max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]:\n",
    "    #             return min_confidence_sum, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "\n",
    "    #         col_idx = sorted_W_row_idxs_sets_confidence_sum_per_col_indices[W_row_idxs_set_idx, curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]].item()\n",
    "    #         W_row_idxs_confidence_sum = sorted_W_row_idxs_sets_confidence_sum_per_col_values[W_row_idxs_set_idx, curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]].item()\n",
    "    #         if min_confidence_sum is not None and W_row_idxs_confidence_sum >= min_confidence_sum:\n",
    "    #             return min_confidence_sum, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "\n",
    "    #         if col_idx % tm.l1.in_dim not in used_W_col_idxs:\n",
    "    #             updated_used_col_idxs = used_W_col_idxs | {col_idx % tm.l1.in_dim}\n",
    "    #             new_max_sum = max_sum - W_row_idxs_confidence_sum if max_sum is not None else None\n",
    "    #             sub_min_confidence_sum , sub_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(W_row_idxs_set_idxs - {W_row_idxs_set_idx}, curr_max_sorted_idx_per_W_row_idxs_set_idxs, updated_used_col_idxs, new_max_sum)\n",
    "\n",
    "    #             if min_confidence_sum is None or (sub_min_confidence_sum is not None and sub_min_confidence_sum + W_row_idxs_confidence_sum < min_confidence_sum):\n",
    "    #                 min_confidence_sum = W_row_idxs_confidence_sum + sub_min_confidence_sum\n",
    "    #                 W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = sub_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "    #                 W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum[W_row_idxs_set_idx] = curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]\n",
    "\n",
    "    #     return min_confidence_sum, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "\n",
    "    # _, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(set(range(len(new_X_row_idxs_per_W_col))), None, set(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "16517892685834588328\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "TRACKING = {0:0}\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_2.txt'\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "\n",
    "def generate_subsets_iterator(set_elements, subset_size):\n",
    "    return combinations(set_elements, subset_size)\n",
    "\n",
    "def generate_powerset_iterator(set_elements):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    return chain.from_iterable(combinations(set_elements,r) for r in range(len(set_elements), -1, -1))\n",
    "\n",
    "def combine_iterators(iterable_one, iterable_two):\n",
    "    for item in iterable_one:\n",
    "        if item:\n",
    "            merged_set = set().union(*item)\n",
    "            yield merged_set\n",
    "\n",
    "    for item in iterable_two:\n",
    "        yield item\n",
    "\n",
    "SEED = None\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "else:\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(seed)\n",
    "\n",
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "\n",
    "tm.forward(train_x)\n",
    "\n",
    "W_row_to_zero_Y_row_idxs = {}\n",
    "W_row_to_one_Y_row_idxs = {}\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    \n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 0).squeeze(1).tolist()\n",
    "    if zero_Y_idxs:\n",
    "        W_row_to_zero_Y_row_idxs[i] = set(zero_Y_idxs)\n",
    "\n",
    "    one_Y_idxs = torch.nonzero(row_Y == 1).squeeze(1).tolist()\n",
    "    if one_Y_idxs:\n",
    "        W_row_to_one_Y_row_idxs[i] = set(one_Y_idxs)\n",
    "\n",
    "W_rows_of_unique_one_Y_row_idxs = set() # this is constructed by assigning one_Y_row_idxs to W columns incrementally (from 0). This will later be changed\n",
    "visited_one_Y_row_idxs = set()\n",
    "\n",
    "W_col_to_new_X_row_idxs = {}\n",
    "for W_row_idx, one_Y_row_idxs in W_row_to_one_Y_row_idxs.items():\n",
    "    tuple_value = tuple(one_Y_row_idxs)\n",
    "    if tuple_value not in visited_one_Y_row_idxs:\n",
    "        visited_one_Y_row_idxs.add(tuple_value)\n",
    "        W_rows_of_unique_one_Y_row_idxs.add(W_row_idx)\n",
    "\n",
    "one_Y_row_state = {W_row: W_row_to_zero_Y_row_idxs.get(W_row, set())  for W_row in W_rows_of_unique_one_Y_row_idxs} # this tracks unresolved zero Y row idxs for each W row idx\n",
    "sorted_one_Y_row_idxs = sorted(list(W_rows_of_unique_one_Y_row_idxs), key=lambda x: len(W_row_to_one_Y_row_idxs[x]), reverse=True) # a heuristical optimization to address the largest one_Y_row_idxs first\n",
    "q = deque(sorted_one_Y_row_idxs)\n",
    "\n",
    "def get_new_X_row_idxs_per_W_col(depth, max_depth, curr_one_Y_row_state, prev_W_row_idx, q):\n",
    "    # the output is of shape [({1,2,3},{4,5,6}), ({2,3},{4,5,1,6}), ...] where ({1,2,3},{4,5,6}) means\n",
    "    # that W[[1,2,3]][0] should be 1 and W[[4,5,6]][0] should be 0 and full_X[[1,2,3]][0] should be 1 \n",
    "    # and full_X[[4,5,6]][0] should be 0\n",
    "    if depth == max_depth or len(curr_one_Y_row_state) == 0:\n",
    "        return [], len(curr_one_Y_row_state) == 0\n",
    "\n",
    "    curr_W_row_idx = prev_W_row_idx\n",
    "    while curr_W_row_idx not in curr_one_Y_row_state and q:\n",
    "        curr_W_row_idx = q.popleft()\n",
    "\n",
    "    curr_one_Y_idxs = W_row_to_one_Y_row_idxs[curr_W_row_idx]\n",
    "    if not curr_one_Y_row_state[curr_W_row_idx]:\n",
    "        updated_one_Y_row_state = copy.deepcopy(curr_one_Y_row_state)\n",
    "        del updated_one_Y_row_state[curr_W_row_idx]\n",
    "        sub_new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(depth+1, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy.deepcopy(q))\n",
    "        if is_solved:\n",
    "            new_X_row_idxs_per_W_col = sub_new_X_row_idxs_per_W_col\n",
    "            new_X_row_idxs_per_W_col.append(( curr_one_Y_idxs, set()))\n",
    "            return new_X_row_idxs_per_W_col, True\n",
    "        else:\n",
    "            return [], False\n",
    "\n",
    "    remaining_q = list(q)\n",
    "    remaining_one_Y_idxs = [W_row_to_one_Y_row_idxs[x] for x in remaining_q if len(W_row_to_one_Y_row_idxs[x] & curr_one_Y_idxs) == 0 and len(W_row_to_one_Y_row_idxs[x] & curr_one_Y_row_state[curr_W_row_idx]) > 0]\n",
    "    for opposite_set in combine_iterators(generate_powerset_iterator(remaining_one_Y_idxs), [curr_one_Y_row_state[curr_W_row_idx]]):\n",
    "        remaining_Y_idxs = set(range(tm.l1.full_X.shape[0])) - (opposite_set | curr_one_Y_idxs)\n",
    "        sub_remaining_one_Y_idxs = [ x for x in remaining_one_Y_idxs if x.issubset(remaining_Y_idxs)]\n",
    "        \n",
    "        for remaining_merged_set in combine_iterators(generate_powerset_iterator(sub_remaining_one_Y_idxs), [set()]):\n",
    "            complement_remaining_Y_subset = remaining_Y_idxs - remaining_merged_set\n",
    "\n",
    "            first_left_W = curr_one_Y_idxs | complement_remaining_Y_subset\n",
    "            first_right_W = opposite_set | remaining_merged_set\n",
    "\n",
    "            second_left_W = curr_one_Y_idxs | remaining_merged_set\n",
    "            second_right_W = opposite_set | complement_remaining_Y_subset\n",
    "\n",
    "            for left_W, right_W in [(first_left_W, first_right_W), (second_left_W, second_right_W)]:\n",
    "                updated_one_Y_row_state = {}\n",
    "                for k,v in curr_one_Y_row_state.items():\n",
    "                    one_Y_idxs = W_row_to_one_Y_row_idxs[k]\n",
    "                    sub_diff = v\n",
    "\n",
    "                    if one_Y_idxs.issubset(left_W):\n",
    "                        sub_diff = v - right_W\n",
    "                    elif one_Y_idxs.issubset(right_W):\n",
    "                        sub_diff = v - left_W\n",
    "                    \n",
    "                    # implicit here is the removal of one_Y_idxs for which there is no unresolved zero Y row idxs left\n",
    "                    if len(sub_diff) > 0:\n",
    "                        updated_one_Y_row_state[k] = sub_diff\n",
    "\n",
    "                sub_new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(depth+1, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy.deepcopy(q))\n",
    "                if is_solved:\n",
    "                    new_X_row_idxs_per_W_col = sub_new_X_row_idxs_per_W_col\n",
    "                    new_X_row_idxs_per_W_col.append((left_W, right_W))\n",
    "                    return new_X_row_idxs_per_W_col, True\n",
    "        \n",
    "    return [], False\n",
    "new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(0, tm.l1.in_dim, one_Y_row_state, q.popleft(), q) # X_row_idxs_per_W_col does not necessarily contain a slot for each col\n",
    "print(is_solved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
