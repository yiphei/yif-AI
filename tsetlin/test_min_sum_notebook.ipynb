{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "8086226574029401178\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_2.txt'\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "\n",
    "def generate_powerset_iterator(set_elements):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    return chain.from_iterable(combinations(set_elements,r) for r in range(len(set_elements), -1, -1))\n",
    "\n",
    "def combine_iterators(iterable_one, iterable_two):\n",
    "    for item in iterable_one:\n",
    "        if item:\n",
    "            merged_set = set().union(*item)\n",
    "            yield merged_set\n",
    "\n",
    "    for item in iterable_two:\n",
    "        yield item\n",
    "\n",
    "SEED = None\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "else:\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(seed)\n",
    "\n",
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "\n",
    "tm.forward(train_x)\n",
    "W_row_to_zero_Y_row_idxs = {}\n",
    "W_row_to_one_Y_row_idxs = {}\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    \n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 0).squeeze(1).tolist()\n",
    "    if zero_Y_idxs:\n",
    "        W_row_to_zero_Y_row_idxs[i] = set(zero_Y_idxs)\n",
    "\n",
    "    one_Y_idxs = torch.nonzero(row_Y == 1).squeeze(1).tolist()\n",
    "    if one_Y_idxs:\n",
    "        W_row_to_one_Y_row_idxs[i] = set(one_Y_idxs)\n",
    "\n",
    "W_rows_of_unique_one_Y_row_idxs = set() # this is constructed by assigning one_Y_row_idxs to W columns incrementally (from 0). This will later be changed\n",
    "visited_one_Y_row_idxs = set()\n",
    "\n",
    "W_col_to_new_X_row_idxs = {}\n",
    "for W_row_idx, one_Y_row_idxs in W_row_to_one_Y_row_idxs.items():\n",
    "    tuple_value = tuple(one_Y_row_idxs)\n",
    "    if tuple_value not in visited_one_Y_row_idxs:\n",
    "        visited_one_Y_row_idxs.add(tuple_value)\n",
    "        W_rows_of_unique_one_Y_row_idxs.add(W_row_idx)\n",
    "\n",
    "one_Y_row_state = {W_row: W_row_to_zero_Y_row_idxs.get(W_row, set())  for W_row in W_rows_of_unique_one_Y_row_idxs} # this tracks unresolved zero Y row idxs for each W row idx\n",
    "sorted_one_Y_row_idxs = sorted(list(W_rows_of_unique_one_Y_row_idxs), key=lambda x: len(W_row_to_one_Y_row_idxs[x]), reverse=True) # a heuristical optimization to address the largest one_Y_row_idxs first\n",
    "q = deque(sorted_one_Y_row_idxs)\n",
    "\n",
    "def get_new_X_row_idxs_per_W_col(depth, max_depth, curr_one_Y_row_state, prev_W_row_idx, q):\n",
    "    # the output is of shape [({1,2,3},{4,5,6}), ({2,3},{4,5,1,6}), ...] where ({1,2,3},{4,5,6}) means\n",
    "    # that W[[1,2,3]][0] should be 1 and W[[4,5,6]][0] should be 0 and full_X[[1,2,3]][0] should be 1 \n",
    "    # and full_X[[4,5,6]][0] should be 0\n",
    "    if depth == max_depth or len(curr_one_Y_row_state) == 0:\n",
    "        return [], len(curr_one_Y_row_state) == 0\n",
    "\n",
    "    curr_W_row_idx = prev_W_row_idx\n",
    "    while curr_W_row_idx not in curr_one_Y_row_state and q:\n",
    "        curr_W_row_idx = q.popleft()\n",
    "\n",
    "    curr_one_Y_idxs = W_row_to_one_Y_row_idxs[curr_W_row_idx]\n",
    "    if not curr_one_Y_row_state[curr_W_row_idx]:\n",
    "        updated_one_Y_row_state = copy.deepcopy(curr_one_Y_row_state)\n",
    "        del updated_one_Y_row_state[curr_W_row_idx]\n",
    "        sub_new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(depth+1, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy.deepcopy(q))\n",
    "        if is_solved:\n",
    "            new_X_row_idxs_per_W_col = sub_new_X_row_idxs_per_W_col\n",
    "            new_X_row_idxs_per_W_col.append(( curr_one_Y_idxs, set()))\n",
    "            return new_X_row_idxs_per_W_col, True\n",
    "        else:\n",
    "            return [], False\n",
    "\n",
    "    remaining_q = list(q)\n",
    "    remaining_one_Y_idxs = [W_row_to_one_Y_row_idxs[x] for x in remaining_q if len(W_row_to_one_Y_row_idxs[x] & curr_one_Y_idxs) == 0 and len(W_row_to_one_Y_row_idxs[x] & curr_one_Y_row_state[curr_W_row_idx]) > 0]\n",
    "    for opposite_set in combine_iterators(generate_powerset_iterator(remaining_one_Y_idxs), [curr_one_Y_row_state[curr_W_row_idx]]):\n",
    "        remaining_Y_idxs = set(range(tm.l1.full_X.shape[0])) - (opposite_set | curr_one_Y_idxs)\n",
    "        sub_remaining_one_Y_idxs = [ x for x in remaining_one_Y_idxs if x.issubset(remaining_Y_idxs)]\n",
    "        \n",
    "        for remaining_merged_set in combine_iterators(generate_powerset_iterator(sub_remaining_one_Y_idxs), [set()]):\n",
    "            complement_remaining_Y_subset = remaining_Y_idxs - remaining_merged_set\n",
    "\n",
    "            first_left_W = curr_one_Y_idxs | complement_remaining_Y_subset\n",
    "            first_right_W = opposite_set | remaining_merged_set\n",
    "\n",
    "            second_left_W = curr_one_Y_idxs | remaining_merged_set\n",
    "            second_right_W = opposite_set | complement_remaining_Y_subset\n",
    "\n",
    "            for left_W, right_W in [(first_left_W, first_right_W), (second_left_W, second_right_W)]:\n",
    "                updated_one_Y_row_state = {}\n",
    "                for k,v in curr_one_Y_row_state.items():\n",
    "                    one_Y_idxs = W_row_to_one_Y_row_idxs[k]\n",
    "                    sub_diff = v\n",
    "\n",
    "                    if one_Y_idxs.issubset(left_W):\n",
    "                        sub_diff = v - right_W\n",
    "                    elif one_Y_idxs.issubset(right_W):\n",
    "                        sub_diff = v - left_W\n",
    "                    \n",
    "                    # implicit here is the removal of one_Y_idxs for which there is no unresolved zero Y row idxs left\n",
    "                    if len(sub_diff) > 0:\n",
    "                        updated_one_Y_row_state[k] = sub_diff\n",
    "\n",
    "                sub_new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(depth+1, max_depth, updated_one_Y_row_state, curr_W_row_idx, copy.deepcopy(q))\n",
    "                if is_solved:\n",
    "                    new_X_row_idxs_per_W_col = sub_new_X_row_idxs_per_W_col\n",
    "                    new_X_row_idxs_per_W_col.append((left_W, right_W))\n",
    "                    return new_X_row_idxs_per_W_col, True\n",
    "        \n",
    "    return [], False\n",
    "\n",
    "new_X_row_idxs_per_W_col, is_solved = get_new_X_row_idxs_per_W_col(0, tm.l1.in_dim, one_Y_row_state, q.popleft(), q) # X_row_idxs_per_W_col does not necessarily contain a slot for each col\n",
    "assert is_solved\n",
    "\n",
    "# new_X_row_idxs_per_W_col provides a valid update of W and full_X that satisfies the expected Y.\n",
    "# However, we assigned one_Y_row_idxs to W columns incrementally (from 0) for simplicity.\n",
    "# Below, we determine the best W column assignment based on W_confidence.\n",
    "\n",
    "W_row_idxs_per_col = defaultdict(lambda: [[], []]) # this represents all one_W_row_idxs and zero_W_row_idxs pairs\n",
    "for W_row_idx, one_Y_row_idxs in W_row_to_one_Y_row_idxs.items():\n",
    "    for W_col_idx, new_X_row_idxs in enumerate(new_X_row_idxs_per_W_col):\n",
    "        if one_Y_row_idxs.issubset(new_X_row_idxs[0]):\n",
    "            W_row_idxs_per_col[W_col_idx][0].append(W_row_idx)\n",
    "        elif one_Y_row_idxs.issubset(new_X_row_idxs[1]):\n",
    "            W_row_idxs_per_col[W_col_idx][1].append(W_row_idx)\n",
    "\n",
    "\n",
    "\n",
    "SEED = None\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "else:\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # print(seed)\n",
    "\n",
    "W_confidence = torch.randint_like(tm.l1.W, 0, 30)\n",
    "\n",
    "\n",
    "# calculate the W_confidence sum of each one_W_row_idxs and zero_W_row_idxs pairs for all W columns\n",
    "W_row_idxs_sets_confidence_sum_per_col = []\n",
    "for W_row_idxs in W_row_idxs_per_col.keys():\n",
    "    sums = W_confidence[W_row_idxs_per_col[W_row_idxs][0]].sum(dim=0)\n",
    "    neg_sum = torch.roll(W_confidence[W_row_idxs_per_col[W_row_idxs][1]].sum(dim=0), shifts = -tm.l1.in_dim, dims=0)\n",
    "    sums += neg_sum\n",
    "    W_row_idxs_sets_confidence_sum_per_col.append(sums)\n",
    "    \n",
    "W_row_idxs_sets_confidence_sum_per_col = torch.stack(W_row_idxs_sets_confidence_sum_per_col)\n",
    "sorted_W_row_idxs_sets_confidence_sum_per_col = torch.sort(W_row_idxs_sets_confidence_sum_per_col, dim=1, descending=False) # sort by increasing sum\n",
    "\n",
    "# Prune W columns\n",
    "opt_values = []\n",
    "opt_indices = []\n",
    "\n",
    "for sum_values, sum_indices in zip(sorted_W_row_idxs_sets_confidence_sum_per_col.values, sorted_W_row_idxs_sets_confidence_sum_per_col.indices):\n",
    "    opt_sums = []\n",
    "    opt_sum_idxs = []\n",
    "    visited_col_idxs = set()\n",
    "    for sum_value, idx in zip(sum_values, sum_indices):\n",
    "        idx_value = idx.item()\n",
    "        if idx_value % tm.l1.in_dim not in visited_col_idxs:\n",
    "            opt_sums.append(sum_value.item())\n",
    "            opt_sum_idxs.append(idx_value)\n",
    "            visited_col_idxs.add(idx_value % tm.l1.in_dim)\n",
    "\n",
    "        if len(visited_col_idxs) == tm.l1.in_dim:\n",
    "            break\n",
    "    \n",
    "    opt_values.append(opt_sums)\n",
    "    opt_indices.append(opt_sum_idxs)\n",
    "\n",
    "sorted_W_row_idxs_sets_confidence_sum_per_col_values = torch.tensor(opt_values)\n",
    "sorted_W_row_idxs_sets_confidence_sum_per_col_indices = torch.tensor(opt_indices) \n",
    "\n",
    "# a heuristical optimization that sorts W columns by increasing offset sum across one_W_row_idxs and zero_W_row_idxs pairs\n",
    "offset_sorted_W_row_idxs_sets_confidence_sum_per_col = sorted_W_row_idxs_sets_confidence_sum_per_col_values - sorted_W_row_idxs_sets_confidence_sum_per_col_values[:, 0].unsqueeze(1) # normalize the sum by subtracting the smallest sum\n",
    "offset_W_row_idxs_sets_confidence_sum_to_cols_dict = defaultdict(list)\n",
    "for col_idx, offset_sums in enumerate(offset_sorted_W_row_idxs_sets_confidence_sum_per_col):\n",
    "    for offset_sum in offset_sums:\n",
    "        offset_W_row_idxs_sets_confidence_sum_to_cols_dict[offset_sum.item()].append(col_idx)\n",
    "sorted_W_row_idxs_sets_confidence_sum = sorted(offset_W_row_idxs_sets_confidence_sum_to_cols_dict.keys())\n",
    "W_row_idxs_set_sequencing = [offset_W_row_idxs_sets_confidence_sum_to_cols_dict[x] for x in sorted_W_row_idxs_sets_confidence_sum] # based on increasing offset W row idxs sets sum\n",
    "W_row_idxs_set_sequencing = [ x for sublist in W_row_idxs_set_sequencing for x in sublist] # flatten\n",
    "\n",
    "def get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(W_row_idxs_set_idxs, max_sorted_idx_per_W_row_idxs_set_idxs, used_W_col_idxs, max_sum):\n",
    "    # This is the core function that determines the best W column assignment based on W_confidence. Before was all preprocessing for a faster algorithm.\n",
    "    # The output shape is {0:1, 1:0, 2:5} where 0:1 means that one_W_row_idxs and zero_W_row_idxs pair indexed at 0 should be assigned to W column 1\n",
    "    \n",
    "    if len(W_row_idxs_set_idxs) == 1:\n",
    "        W_row_idxs_set_idx = list(W_row_idxs_set_idxs)[0]\n",
    "        max_sorted_idx = max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx] if max_sorted_idx_per_W_row_idxs_set_idxs is not None else tm.l1.in_dim - 1\n",
    "        \n",
    "        for sorted_idx in range(max_sorted_idx + 1):\n",
    "            col_idx = sorted_W_row_idxs_sets_confidence_sum_per_col_indices[W_row_idxs_set_idx, sorted_idx].item()\n",
    "            W_row_idxs_confidence_sum = sorted_W_row_idxs_sets_confidence_sum_per_col_values[W_row_idxs_set_idx, sorted_idx].item()\n",
    "\n",
    "            if max_sum is not None and W_row_idxs_confidence_sum >= max_sum:\n",
    "                return None, None\n",
    "            if col_idx % tm.l1.in_dim not in used_W_col_idxs:\n",
    "                return W_row_idxs_confidence_sum, {W_row_idxs_set_idx: sorted_idx}\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    curr_max_sorted_idx_per_W_row_idxs_set_idxs = [-1] * len(new_X_row_idxs_per_W_col)\n",
    "    min_confidence_sum = max_sum\n",
    "    W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = None\n",
    "\n",
    "    for i in range(len(W_row_idxs_set_sequencing)):\n",
    "        W_row_idxs_set_idx = W_row_idxs_set_sequencing[i]\n",
    "        if W_row_idxs_set_idx not in W_row_idxs_set_idxs:\n",
    "            continue\n",
    "\n",
    "        curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx] += 1\n",
    "        if max_sorted_idx_per_W_row_idxs_set_idxs is not None and curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx] > max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]:\n",
    "            return min_confidence_sum, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "\n",
    "        col_idx = sorted_W_row_idxs_sets_confidence_sum_per_col_indices[W_row_idxs_set_idx, curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]].item()\n",
    "        W_row_idxs_confidence_sum = sorted_W_row_idxs_sets_confidence_sum_per_col_values[W_row_idxs_set_idx, curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]].item()\n",
    "        if min_confidence_sum is not None and W_row_idxs_confidence_sum >= min_confidence_sum:\n",
    "            return min_confidence_sum, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "\n",
    "        if col_idx % tm.l1.in_dim not in used_W_col_idxs:\n",
    "            updated_used_col_idxs = used_W_col_idxs | {col_idx % tm.l1.in_dim}\n",
    "            new_max_sum = max_sum - W_row_idxs_confidence_sum if max_sum is not None else None\n",
    "            sub_min_confidence_sum , sub_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(W_row_idxs_set_idxs - {W_row_idxs_set_idx}, curr_max_sorted_idx_per_W_row_idxs_set_idxs, updated_used_col_idxs, new_max_sum)\n",
    "\n",
    "            if (min_confidence_sum is None and sub_min_confidence_sum is not None) or (sub_min_confidence_sum is not None and sub_min_confidence_sum + W_row_idxs_confidence_sum < min_confidence_sum):\n",
    "                min_confidence_sum = W_row_idxs_confidence_sum + sub_min_confidence_sum\n",
    "                W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum = sub_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "                W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum[W_row_idxs_set_idx] = curr_max_sorted_idx_per_W_row_idxs_set_idxs[W_row_idxs_set_idx]\n",
    "\n",
    "    return min_confidence_sum, W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum\n",
    "\n",
    "min_sum_1, sol_dict_1 = get_W_row_idxs_set_idx_to_sorted_col_idx_w_min_confidence_sum(set(range(len(new_X_row_idxs_per_W_col))), None, set(), None)\n",
    "\n",
    "print(min_sum_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
