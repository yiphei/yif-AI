{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Working seeds\n",
    "# SEED = 9496461801973866405 # this works\n",
    "# SEED = 3158761121381184149\n",
    "# SEED = 4954668783344399908\n",
    "# SEED = 2763762423645165824\n",
    "\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_1.txt'\n",
    "SEED = 1158604801931000331\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "else:\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(seed)\n",
    "\n",
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "out_1 = tm.forward(train_x)\n",
    "tm.l1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1, 3, 4, 6, 7},\n",
       " {0, 1, 2, 4, 5, 6, 7},\n",
       " {0, 1, 2, 3, 5, 7},\n",
       " {0, 2, 4, 5},\n",
       " {0, 1, 4, 5, 6, 7}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "zero_Y_row_idxs_per_W_row = []\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 0).squeeze(1).tolist()\n",
    "    zero_Y_row_idxs_per_W_row.append(set(zero_Y_idxs))\n",
    "zero_Y_row_idxs_per_W_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2, 5}, {3}, {4, 6}, {1, 3, 6, 7}, {2, 3}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "one_Y_row_idxs_per_W_row = []\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 1).squeeze(1).tolist()\n",
    "    one_Y_row_idxs_per_W_row.append(set(zero_Y_idxs))\n",
    "one_Y_row_idxs_per_W_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: {2, 5}, 1: {3}, 2: {4, 6}, 3: {1, 3, 6, 7}, 4: {2, 3}},\n",
       " {0: {0, 1, 3, 4, 6, 7},\n",
       "  1: {0, 1, 2, 4, 5, 6, 7},\n",
       "  2: {0, 1, 2, 3, 5, 7},\n",
       "  3: {0, 2, 4, 5},\n",
       "  4: {0, 1, 4, 5, 6, 7}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "unique_idxs = set()\n",
    "visited_ones = set()\n",
    "for i,x in enumerate(one_Y_row_idxs_per_W_row):\n",
    "    tuple_x = tuple(x)\n",
    "    if tuple_x not in visited_ones:\n",
    "        visited_ones.add(tuple_x)\n",
    "        unique_idxs.add(i)\n",
    "\n",
    "\n",
    "row_idx_to_one_values = {x: one_Y_row_idxs_per_W_row[x] for x in unique_idxs}\n",
    "tracking = {x: zero_Y_row_idxs_per_W_row[x] for x in unique_idxs}\n",
    "\n",
    "row_idx_to_one_values, tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 2, 4, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_one_Y_row_idxs = sorted(row_idx_to_one_values, key=lambda x: len(row_idx_to_one_values[x]), reverse=True)\n",
    "sorted_one_Y_row_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(), (4,), (5,), (4, 5)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import combinations, chain\n",
    "\n",
    "def generate_subsets(set_elements, combination_size):\n",
    "    return list(combinations(set_elements, combination_size))\n",
    "\n",
    "def generate_powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "\n",
    "q = queue.Queue()\n",
    "for i in sorted_one_Y_row_idxs:\n",
    "    q.put(i)\n",
    "\n",
    "def funca(depth, max_depth, current_solution, prev_clause_idx, q):\n",
    "    if depth == max_depth or len(current_solution) == 0:\n",
    "        return [], len(current_solution) == 0\n",
    "\n",
    "    curr_clause_idx = prev_clause_idx\n",
    "    while curr_clause_idx not in current_solution and q:\n",
    "        curr_clause_idx = q.get()\n",
    "\n",
    "    curr_clause = row_idx_to_one_values[curr_clause_idx]\n",
    "    min_zero_rows = math.ceil((tm.l1.full_X.shape[0] - len(current_solution[curr_clause_idx])) / (max_depth - depth))\n",
    "    min_zero_subset_tuples = generate_subsets(tracking[curr_clause_idx], min_zero_rows)\n",
    "\n",
    "    for min_zero_subset_tuple in min_zero_subset_tuples:\n",
    "        min_zero_subset = set(min_zero_subset_tuple)\n",
    "        remaining_values = set(range(tm.l1.full_X.shape[0])) - (min_zero_subset | curr_clause)\n",
    "        remaining_subsets_tuples = generate_powerset(remaining_values)\n",
    "        for remaining_subset_tuple in remaining_subsets_tuples:\n",
    "            remaining_subset = set(remaining_subset_tuple)\n",
    "            opposite_remaining_subset = remaining_values - remaining_subset\n",
    "\n",
    "            #add remaining with the curr_clause\n",
    "            left_clause = curr_clause | set(remaining_subset_tuple)\n",
    "            right_clause = min_zero_subset | opposite_remaining_subset\n",
    "\n",
    "            updated_solution = {}\n",
    "            for k,v in current_solution.items():\n",
    "                corresponding_clause = one_Y_row_idxs_per_W_row[k]\n",
    "                if corresponding_clause.issubset(left_clause):\n",
    "                    sub = v - right_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "                elif corresponding_clause.issubset(right_clause):\n",
    "                    sub = v - left_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "\n",
    "            next_layers, solved = funca(depth+1, max_depth, updated_solution, curr_clause_idx, q)\n",
    "            if solved:\n",
    "                return_layers = next_layers\n",
    "                return_layers.append((left_clause, right_clause))\n",
    "                return return_layers, True\n",
    "\n",
    "            #add remaining with the opposite\n",
    "            left_clause = curr_clause | set(opposite_remaining_subset)\n",
    "            right_clause = min_zero_subset | remaining_subset_tuple\n",
    "\n",
    "            updated_solution = {}\n",
    "            for k,v in current_solution.items():\n",
    "                corresponding_clause = one_Y_row_idxs_per_W_row[k]\n",
    "                if corresponding_clause.issubset(left_clause):\n",
    "                    sub = v - right_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "                elif corresponding_clause.issubset(right_clause):\n",
    "                    sub = v - left_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "\n",
    "            next_layers, solved = funca(depth+1, max_depth, updated_solution, curr_clause_idx, q)\n",
    "            if solved:\n",
    "                return_layers = next_layers\n",
    "                return_layers.append((left_clause, right_clause))\n",
    "                return return_layers, True\n",
    "            \n",
    "    return [], False\n",
    "\n",
    "funca(0, tm.l1.in_dim, tracking, q.get(), q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
