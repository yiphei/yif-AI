{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Working seeds\n",
    "# SEED = 9496461801973866405 # this works\n",
    "# SEED = 3158761121381184149\n",
    "# SEED = 4954668783344399908\n",
    "# SEED = 2763762423645165824\n",
    "\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_1.txt'\n",
    "SEED = 1158604801931000331\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "else:\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(seed)\n",
    "\n",
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "out_1 = tm.forward(train_x)\n",
    "tm.l1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1, 3, 4, 6, 7},\n",
       " {0, 1, 2, 4, 5, 6, 7},\n",
       " {0, 1, 2, 3, 5, 7},\n",
       " {0, 2, 4, 5},\n",
       " {0, 1, 4, 5, 6, 7}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "zero_Y_row_idxs_per_W_row = []\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 0).squeeze(1).tolist()\n",
    "    zero_Y_row_idxs_per_W_row.append(set(zero_Y_idxs))\n",
    "zero_Y_row_idxs_per_W_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2, 5}, {3}, {4, 6}, {1, 3, 6, 7}, {2, 3}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "one_Y_row_idxs_per_W_row = []\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 1).squeeze(1).tolist()\n",
    "    one_Y_row_idxs_per_W_row.append(set(zero_Y_idxs))\n",
    "one_Y_row_idxs_per_W_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: {2, 5}, 1: {3}, 2: {4, 6}, 3: {1, 3, 6, 7}, 4: {2, 3}},\n",
       " {0: {0, 1, 3, 4, 6, 7},\n",
       "  1: {0, 1, 2, 4, 5, 6, 7},\n",
       "  2: {0, 1, 2, 3, 5, 7},\n",
       "  3: {0, 2, 4, 5},\n",
       "  4: {0, 1, 4, 5, 6, 7}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_idxs = set()\n",
    "visited_ones = set()\n",
    "for i,x in enumerate(one_Y_row_idxs_per_W_row):\n",
    "    tuple_x = tuple(x)\n",
    "    if tuple_x not in visited_ones:\n",
    "        visited_ones.add(tuple_x)\n",
    "        unique_idxs.add(i)\n",
    "\n",
    "\n",
    "row_idx_to_one_values = {x: one_Y_row_idxs_per_W_row[x] for x in unique_idxs}\n",
    "tracking = {x: zero_Y_row_idxs_per_W_row[x] for x in unique_idxs}\n",
    "\n",
    "row_idx_to_one_values, tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 2, 4, 1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_one_Y_row_idxs = sorted(row_idx_to_one_values, key=lambda x: len(row_idx_to_one_values[x]), reverse=True)\n",
    "sorted_one_Y_row_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([({4, 5, 6}, {0, 1, 2, 3, 7}),\n",
       "  ({2, 3, 5}, {0, 1, 4, 6, 7}),\n",
       "  ({1, 3, 6, 7}, {0, 2, 4, 5})],\n",
       " True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from collections import deque\n",
    "\n",
    "def generate_subsets(set_elements, combination_size):\n",
    "    return [set(x) for x in list(combinations(set_elements, combination_size))]\n",
    "\n",
    "def generate_powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return [set(x) for x in list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))]\n",
    "\n",
    "\n",
    "q = deque(sorted_one_Y_row_idxs)\n",
    "\n",
    "def funca(depth, max_depth, current_solution, prev_clause_idx, q):\n",
    "    if depth == max_depth or len(current_solution) == 0 or not q:\n",
    "        return [], len(current_solution) == 0\n",
    "\n",
    "    curr_clause_idx = prev_clause_idx\n",
    "    while curr_clause_idx not in current_solution and q:\n",
    "        curr_clause_idx = q.popleft()\n",
    "\n",
    "    curr_clause = row_idx_to_one_values[curr_clause_idx]\n",
    "    min_zero_rows = math.ceil((tm.l1.full_X.shape[0] - len(current_solution[curr_clause_idx])) / (max_depth - depth))\n",
    "    min_zero_subsets = generate_subsets(current_solution[curr_clause_idx], min(min_zero_rows, len(current_solution[curr_clause_idx])))\n",
    "\n",
    "    ordered_min_zero_subsets = []\n",
    "    remaining_q = list(q)\n",
    "    for idx in remaining_q:\n",
    "        clause = row_idx_to_one_values[idx]\n",
    "        if len(clause) == min_zero_rows and len(clause & curr_clause) == 0 and len(clause & current_solution[curr_clause_idx]) > 0:\n",
    "            ordered_min_zero_subsets.append(clause)\n",
    "\n",
    "    for subset in min_zero_subsets:\n",
    "        if subset not in ordered_min_zero_subsets:\n",
    "            ordered_min_zero_subsets.append(subset)\n",
    "\n",
    "    for min_zero_subset in ordered_min_zero_subsets:\n",
    "        remaining_values = set(range(tm.l1.full_X.shape[0])) - (min_zero_subset | curr_clause)\n",
    "        remaining_subsets = generate_powerset(remaining_values)\n",
    "        remaining_subsets.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "        remaining_subsets_ordered = []\n",
    "        for idx in remaining_q:\n",
    "            clause = row_idx_to_one_values[idx]\n",
    "            if clause.issubset(remaining_values):\n",
    "                remaining_subsets_ordered.append(clause)\n",
    "\n",
    "        for subset in remaining_subsets:\n",
    "            if subset not in remaining_subsets_ordered:\n",
    "                remaining_subsets_ordered.append(subset)\n",
    "\n",
    "        for remaining_subset in remaining_subsets_ordered:\n",
    "            opposite_remaining_subset = remaining_values - remaining_subset\n",
    "\n",
    "            #add remaining with the opposite\n",
    "            left_clause = curr_clause | opposite_remaining_subset\n",
    "            right_clause = min_zero_subset | remaining_subset\n",
    "\n",
    "            updated_solution = {}\n",
    "            for k,v in current_solution.items():\n",
    "                corresponding_clause = one_Y_row_idxs_per_W_row[k]\n",
    "                if corresponding_clause.issubset(left_clause):\n",
    "                    sub = v - right_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "                elif corresponding_clause.issubset(right_clause):\n",
    "                    sub = v - left_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "                else:\n",
    "                    updated_solution[k] = v\n",
    "\n",
    "            next_layers, solved = funca(depth+1, max_depth, updated_solution, curr_clause_idx, copy.deepcopy(q))\n",
    "            if solved:\n",
    "                return_layers = next_layers\n",
    "                return_layers.append((left_clause, right_clause))\n",
    "                return return_layers, True\n",
    "            \n",
    "            #add remaining with the curr_clause\n",
    "            left_clause = curr_clause | remaining_subset\n",
    "            right_clause = min_zero_subset | opposite_remaining_subset\n",
    "\n",
    "            updated_solution = {}\n",
    "            for k,v in current_solution.items():\n",
    "                corresponding_clause = one_Y_row_idxs_per_W_row[k]\n",
    "                if corresponding_clause.issubset(left_clause):\n",
    "                    sub = v - right_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "                elif corresponding_clause.issubset(right_clause):\n",
    "                    sub = v - left_clause\n",
    "                    if len(sub) > 0:\n",
    "                        updated_solution[k] = sub\n",
    "                else:\n",
    "                    updated_solution[k] = v\n",
    "\n",
    "            next_layers, solved = funca(depth+1, max_depth, updated_solution, curr_clause_idx, copy.deepcopy(q))\n",
    "            if solved:\n",
    "                return_layers = next_layers\n",
    "                return_layers.append((left_clause, right_clause))\n",
    "                return return_layers, True\n",
    "\n",
    "    return [], False\n",
    "\n",
    "cols, solved = funca(0, tm.l1.in_dim, tracking, q.popleft(), q)\n",
    "cols, solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 0],\n",
       "        [0, 1, 0, 1, 0, 1],\n",
       "        [0, 1, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1],\n",
       "        [1, 1, 0, 0, 0, 1],\n",
       "        [1, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_full_X = torch.zeros_like(tm.l1.full_X)\n",
    "for i, col in enumerate(cols):\n",
    "    new_full_X[list(col[0]), i] = 1\n",
    "    new_full_X[list(col[1]), i + tm.l1.in_dim] = 1\n",
    "new_full_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 1],\n",
       "        [0, 1, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_W = torch.zeros_like(tm.l1.W)\n",
    "for row_idx, x in enumerate(one_Y_row_idxs_per_W_row):\n",
    "   for i, col in enumerate(cols):\n",
    "        col_left = col[0]\n",
    "        col_right = col[1]\n",
    "        if x.issubset(col_left):\n",
    "            new_W[row_idx, i] = 1\n",
    "        elif x.issubset(col_right):\n",
    "            new_W[row_idx, i + tm.l1.in_dim] = 1\n",
    "            \n",
    "new_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsetlin import TsetlinBase\n",
    "\n",
    "tb = TsetlinBase()\n",
    "y2 = tb.conjunction_mul(new_full_X.unsqueeze(1), new_W)\n",
    "(y2 == tm.l1.out).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
