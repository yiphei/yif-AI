{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from tsetlin import TsetlinMachine\n",
    "import torch\n",
    "import random\n",
    "\n",
    "DATASET_DIR = '../datasets/'\n",
    "DATA_FILE = 'bit_1.txt'\n",
    "# SEED = 5779661865816281544\n",
    "SEED = 5779661865816281544\n",
    "\n",
    "text_rows = open(f'{DATASET_DIR}{DATA_FILE}', 'r').read().splitlines()\n",
    "dataset = [ [int(num) for num in row.split(',')] for row in text_rows]\n",
    "tensor_dataset = torch.tensor(dataset)\n",
    "train_x = tensor_dataset[:, :-1]\n",
    "train_y = tensor_dataset[:, -1]\n",
    "\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "else:\n",
    "    seed = int.from_bytes(os.urandom(8), byteorder=\"big\", signed=False)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(seed)\n",
    "\n",
    "tm = TsetlinMachine(train_x.shape[1], 5)\n",
    "\n",
    "out_1 = tm.forward(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsetlin import TsetlinBase\n",
    "\n",
    "b = TsetlinBase()\n",
    "out_check = b.conjunction_mul(tm.l1.full_X.unsqueeze(1), tm.l1.W)\n",
    "assert torch.all(out_check == tm.l1.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 0, 0, 1, 1],\n",
       "         [0, 1, 1, 1, 0, 0],\n",
       "         [1, 0, 1, 0, 1, 0],\n",
       "         [1, 1, 1, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 0, 0]]),\n",
       " tensor([[1, 0, 0, 0, 1, 1],\n",
       "         [1, 0, 1, 0, 1, 0],\n",
       "         [1, 1, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 1, 0, 1, 0, 1],\n",
       "         [0, 0, 1, 1, 1, 0],\n",
       "         [0, 1, 1, 1, 0, 0]]),\n",
       " tensor([[1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 1]]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.l1.W, tm.l1.full_X, tm.l1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{1, 2, 3, 4, 5, 6, 7},\n",
       "  {0, 1, 2, 3, 4, 5, 6},\n",
       "  {0, 2, 3, 4, 5, 6, 7},\n",
       "  {0, 1, 2, 4, 5, 6, 7},\n",
       "  {0, 1, 2, 3, 4, 5, 6}],\n",
       " [{0}, {7}, {1}, {3}, {7}])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_Y_row_idxs_per_W_row = []\n",
    "one_Y_row_idxs_per_W_row = []\n",
    "for i in range(tm.l1.W.shape[0]):\n",
    "    row_Y = tm.l1.out[:, i]\n",
    "    \n",
    "    zero_Y_idxs = torch.nonzero(row_Y == 0).squeeze(1).tolist()\n",
    "    zero_Y_row_idxs_per_W_row.append(set(zero_Y_idxs))\n",
    "\n",
    "    one_Y_idxs = torch.nonzero(row_Y == 1).squeeze(1).tolist()\n",
    "    one_Y_row_idxs_per_W_row.append(set(one_Y_idxs))\n",
    "\n",
    "zero_Y_row_idxs_per_W_row, one_Y_row_idxs_per_W_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({0, 2, 4, 5, 6, 7}, {1, 3}), ({0, 3, 7}, {1, 2, 4, 5, 6}), ({0, 4, 5, 6}, {1, 2, 3, 7})]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from collections import deque\n",
    "\n",
    "def generate_subsets(set_elements, subset_size):\n",
    "    return [set(x) for x in list(combinations(set_elements, subset_size))]\n",
    "\n",
    "def generate_powerset(set_elements):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(set_elements)\n",
    "    return [set(x) for x in list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))]\n",
    "\n",
    "unique_one_Y_row_idxs = set()\n",
    "visited_ones = set()\n",
    "for i,x in enumerate(one_Y_row_idxs_per_W_row):\n",
    "    if x:\n",
    "        tuple_x = tuple(x)\n",
    "        if tuple_x not in visited_ones:\n",
    "            visited_ones.add(tuple_x)\n",
    "            unique_one_Y_row_idxs.add(i)\n",
    "\n",
    "\n",
    "tracking = {x: zero_Y_row_idxs_per_W_row[x] for x in unique_one_Y_row_idxs}\n",
    "sorted_one_Y_row_idxs = sorted(list(unique_one_Y_row_idxs), key=lambda x: len(one_Y_row_idxs_per_W_row[x]), reverse=True)\n",
    "q = deque(sorted_one_Y_row_idxs)\n",
    "\n",
    "def recursive_helper(depth, max_depth, current_solution, prev_W_row_idx, q):\n",
    "    if depth == max_depth or len(current_solution) == 0:\n",
    "        return [], len(current_solution) == 0\n",
    "\n",
    "    curr_W_row_idx = prev_W_row_idx\n",
    "    while curr_W_row_idx not in current_solution and q:\n",
    "        curr_W_row_idx = q.popleft()\n",
    "\n",
    "    curr_one_Y_idxs = one_Y_row_idxs_per_W_row[curr_W_row_idx]\n",
    "    min_zero_Y_idxs_len = math.ceil(len(current_solution[curr_W_row_idx]) / (max_depth - depth))\n",
    "    min_zero_Y_subsets = generate_subsets(current_solution[curr_W_row_idx], min(min_zero_Y_idxs_len, len(current_solution[curr_W_row_idx])))\n",
    "\n",
    "    ordered_min_zero_Y_subsets = []\n",
    "    remaining_q = list(q)\n",
    "    for idx in remaining_q:\n",
    "        one_Y_idx = one_Y_row_idxs_per_W_row[idx]\n",
    "        if len(one_Y_idx) == min_zero_Y_idxs_len and len(one_Y_idx & curr_one_Y_idxs) == 0 and len(one_Y_idx & current_solution[curr_W_row_idx]) > 0:\n",
    "            ordered_min_zero_Y_subsets.append(one_Y_idx)\n",
    "\n",
    "    for subset in min_zero_Y_subsets:\n",
    "        if subset not in ordered_min_zero_Y_subsets:\n",
    "            ordered_min_zero_Y_subsets.append(subset)\n",
    "\n",
    "    for min_zero_Y_subset in ordered_min_zero_Y_subsets:\n",
    "        remaining_Y_idxs = set(range(tm.l1.full_X.shape[0])) - (min_zero_Y_subset | curr_one_Y_idxs)\n",
    "        remaining_Y_subsets = generate_powerset(remaining_Y_idxs)\n",
    "        remaining_Y_subsets.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "        remaining_Y_subsets_ordered = []\n",
    "        for idx in remaining_q:\n",
    "            one_Y_idx = one_Y_row_idxs_per_W_row[idx]\n",
    "            if one_Y_idx.issubset(remaining_Y_idxs):\n",
    "                remaining_Y_subsets_ordered.append(one_Y_idx)\n",
    "\n",
    "        for subset in remaining_Y_subsets:\n",
    "            if subset not in remaining_Y_subsets_ordered:\n",
    "                remaining_Y_subsets_ordered.append(subset)\n",
    "\n",
    "        for remaining_Y_subset in remaining_Y_subsets_ordered:\n",
    "            opposite_remaining_Y_subset = remaining_Y_idxs - remaining_Y_subset\n",
    "\n",
    "            #add remaining with the opposite\n",
    "            first_left_W = curr_one_Y_idxs | opposite_remaining_Y_subset\n",
    "            first_right_W = min_zero_Y_subset | remaining_Y_subset\n",
    "\n",
    "            second_left_W = curr_one_Y_idxs | remaining_Y_subset\n",
    "            second_right_W = min_zero_Y_subset | opposite_remaining_Y_subset\n",
    "\n",
    "            for left_W, right_W in [(first_left_W, first_right_W), (second_left_W, second_right_W)]:\n",
    "                updated_solution = {}\n",
    "                for k,v in current_solution.items():\n",
    "                    one_Y_idxs = one_Y_row_idxs_per_W_row[k]\n",
    "                    if one_Y_idxs.issubset(left_W):\n",
    "                        sub = v - right_W\n",
    "                        if len(sub) > 0:\n",
    "                            updated_solution[k] = sub\n",
    "                    elif one_Y_idxs.issubset(right_W):\n",
    "                        sub = v - left_W\n",
    "                        if len(sub) > 0:\n",
    "                            updated_solution[k] = sub\n",
    "                    else:\n",
    "                        updated_solution[k] = v\n",
    "\n",
    "                next_cols, solved = recursive_helper(depth+1, max_depth, updated_solution, curr_W_row_idx, copy.deepcopy(q))\n",
    "                if solved:\n",
    "                    combined_cols = next_cols\n",
    "                    combined_cols.append((left_W, right_W))\n",
    "                    return combined_cols, True\n",
    "                \n",
    "    return [], False\n",
    "\n",
    "cols, solved = recursive_helper(0, tm.l1.in_dim, tracking, q.popleft(), q)\n",
    "print(cols)\n",
    "assert solved\n",
    "\n",
    "new_W = torch.zeros_like(tm.l1.W)\n",
    "for row_idx, x in enumerate(one_Y_row_idxs_per_W_row):\n",
    "    if x:\n",
    "        for i, col in enumerate(cols):\n",
    "            col_left = col[0]\n",
    "            col_right = col[1]\n",
    "            if x.issubset(col_left):\n",
    "                new_W[row_idx, i] = 1\n",
    "            elif x.issubset(col_right):\n",
    "                new_W[row_idx, i + tm.l1.in_dim] = 1\n",
    "\n",
    "new_full_X = torch.zeros_like(tm.l1.full_X)\n",
    "for i, col in enumerate(cols):\n",
    "    new_full_X[list(col[0]), i] = 1\n",
    "    new_full_X[list(col[1]), i + tm.l1.in_dim] = 1 # this is not needed in the actual code because we only pass the left side back to the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [1, 0, 0, 0, 1, 1],\n",
       "         [0, 1, 0, 1, 0, 1],\n",
       "         [1, 0, 1, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 1, 0],\n",
       "         [1, 1, 0, 0, 0, 1]]),\n",
       " tensor([[1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 1, 0, 1, 0, 1],\n",
       "         [1, 1, 0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_full_X, new_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "from tsetlin import TsetlinBase\n",
    "\n",
    "b = TsetlinBase()\n",
    "y_2 = b.conjunction_mul(new_full_X.unsqueeze(1), new_W)\n",
    "print(y_2)\n",
    "assert torch.all(y_2 == tm.l1.out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
