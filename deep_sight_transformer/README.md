# DeepSight (WIP)
> NB: LaTeX here is optimized for Github's Markdown, so please view it on Github. Also, Safari does not render Github's LaTeX well, so Chrome is advised.

Virtually all autoregressive models are trained with the singular objective of next token prediction. They don't have an explicit objective to think beyond the next token (though they implicitly do). Here, I present a new transformer model, DeepSight, that includes this additional objective of planning beyond the next token. DeepSight beats, with fewer parameters, a canonical decoder-only transformer, both in train and validation loss.

## Motivations

Despite being trained on next token prediction, autoregressive transformer models do develop abilities to plan beyond the next token because of the attention mechanism. However, this ability is rather weak and many failure modes can be attributed to this weakness.

This project explores how planning many steps beyond the next token can be formulated as an objective function, in addition to the regular next token prediction. The new model presented here, DeepSight, implements a planning objective function that takes into account n tokens in the future. This, along with novel components described in sections that follow, beats – with fewer parameters – the baseline of a decoder-only transformer, both in train and validation loss.

## Architecture

At the high level, the architecture consists of an autoregressive encoder-decoder (like encoder_decoder_transformer). The encoder-decoder separation is necessary to the formulation of the planning objective.

### Encoder-Decoder

> This section reiterates the same architecture presented in encoder_decoder_transformer.

In the canonical encoder-decoder transformer, the encoder runs once on an input, and then the decoder runs auto-regressively on its own output while attending to the encoder output. It looks like this

<div align="center">
  <img src="assets/self_canon_diagram.svg" alt="diagram" width="500">
</div>
<br>

To use this architecture for an end-to-end auto-regressive task, the encoder and decoder are adapted to run serially on each new model input. The encoder generates an output and the decoder generates the next token while attending to the encoder output. When a new input is formed with the last decoder output, it gets fed back to the model, which reruns the encoder and decoder. To make this work, the encoder's attention has to be masked. The new architecture is shown in the figure below.

<div align="center">
    <img src="assets/self_new_diagram.svg"
         alt="diagram" height="900">
</div>
<br>

Stated alternatively, the new architecture takes a regular decoder-only architecture with $L$ layers and makes the last $L_{decoder}$ layers perform both self-attention and cross-attention on the output of the first $L_{encoder}$ layers. 

When transitioning from encoder to decoder, the input to the first decoder layer is generated by a linear pass on the encoder output. For simplicity, the new architecture consists of an equal number of encoder and decoder layers.

### Future loss

In the canonical decoder-encoder model, the loss function is evaluated over the decoder's output (itself being a function of the encoder's output). In this implementation, a new future loss is introduced, in addition to the regular (decoder) loss. This loss serves to push the model to predict the future context, the context from the next token until the nth, which in turn should help develop planning abilities in the model.

The first thing to observe is that planning for the future requires it being captured by the latent representation. So we know that whatever the model does for planning, it will be reflected in the latent representations. 



The first thing to observe is that we do have the future context available for most tokens. The crux is how to create the "ground truth" and how to create a loss over it. On the former, we know that we can aggregate input embeddings to generate some future embeddings. On the latter, we leverage the encoder-decoder separation.



In a encoder-decoder transformer, there is a distinct separation between understanding and prediction. The encoder focuses more on understanding, and the decoder more on prediction. This separation is convenient because it allows us to introduce inductive bias to one without drastically affecting the other. In our case, it is easier to introduce inductive bias to the encoder than decoder. We ask the encoder to not just understand the existing context but also future context. In other words, instead of each latent representation $h_t$ representing the entire context from $t$ to $1$, we can extend the representation to include tokens from $t$ to $t+n$.


## Results

> All training runs below were done on a wikipedia dataset for 9k steps on a single A100 GPU, unless otherwise stated.
> 
> Implementation of decoder-only transformer model (baseline) can be found in the `baseline_transformer` directory in this repo


## Next steps

These are some further things to look forward to:
TODO

## Conclusions

---
## Appendix
### Run configs
TODO