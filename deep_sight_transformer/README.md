# DeepSight (WIP)
> NB: LaTeX here is optimized for Github's Markdown, so please view it on Github. Also, Safari does not render Github's LaTeX well, so Chrome is advised.

Virtually all autoregressive models are trained with the singular objective of next token prediction. They don't possess an explicit objective to think – or better, plan – beyond the next token (though they implicitly do). Here, I present a new transformer model, DeepSight, that includes an explicit objective of planning beyond the next token, in addition to next token prediction. DeepSight beats, with fewer parameters, a canonical decoder-only transformer, both in train and validation loss.

## Motivations

Despite being trained on next token prediction, autoregressive transformer models do develop abilities to plan beyond the next token because of the attention mechanism. However, this ability is rather weak and many failure modes can be attributed to this weakness. Note that I restrict planning to whatever happens in a single forward pass. Indeed, models can exhibit better planning at the prompt level once you introduce chaining or other clever orchestration logic. 

This project explores how planning many steps beyond the next token can be formulated as an objective function, in addition to the regular next token prediction. The (perhaps antropomorphic) intuition is that deliberate planning will improve downstream next token prediction.

## Architecture

At the high level, the architecture consists of an autoregressive encoder-decoder (like encoder_decoder_transformer). The encoder-decoder separation is necessary to the formulation of the planning objective.

### Encoder-Decoder

> This section reiterates the same architecture presented in encoder_decoder_transformer.

In the canonical encoder-decoder transformer, the encoder runs once on an input, and then the decoder runs auto-regressively on its own output while attending to the encoder output. It looks like this

<div align="center">
  <img src="assets/self_canon_diagram.svg" alt="diagram" width="500">
</div>
<br>

To use this architecture for an end-to-end auto-regressive task, the encoder and decoder are adapted to run serially on each new model input. The encoder generates an output and the decoder generates the next token while attending to the encoder output. When a new input is formed with the last decoder output, it gets fed back to the model, which reruns the encoder and decoder. To make this work, the encoder's attention has to be masked. The new architecture is shown in the figure below.

<div align="center">
    <img src="assets/self_new_diagram.svg"
         alt="diagram" height="900">
</div>
<br>

Stated alternatively, the new architecture takes a regular decoder-only architecture with $L$ layers and makes the last $L_{decoder}$ layers perform both self-attention and cross-attention on the output of the first $L_{encoder}$ layers. 

When transitioning from encoder to decoder, the input to the first decoder layer is generated by a linear pass on the encoder output. For simplicity, the new architecture consists of an equal number of encoder and decoder layers.

### Future loss

To improve the model's planning abilities, an explicit objective function must be added. As for any objective function, you need: 1) model output, 2) ground truth, and 3) a minimization function.

For 1), first note that it is hard to make one output fulfill two predictive functions, so we need a different output than the one used for next token prediction. Second, observe that this output must be used by the model to produce the downstream next token prediction output, so it must be play an important role in the computational graph. In a decoder-only transformer, your choice is basically a hidden state, which is too transient. Or you can do something more complicated but risk bloating the model. However, in an encoder-decoder transformer, there are two natural distinct outputs, and the decoder attends to the encoder output in every single layer. Indeed, planning may be viewed as an extension of understanding, which is the focus of the encoder. Therefore, the encoder output is the model output used for the planning objective function.

For 2), we need to define what it means to plan ahead such that it is compatible with deep learning. The easiest and best way to define any model behavior is as a prediction. Prediction of what? A plan is too vague to work with. Remember, though, that transformers are excellent at contextual understanding, more so in the encoder layers. Normally, the contextual understanding of any hidden state $h_{t}$ spans from token $x_t$ to $x_1$, but that can be extended it to include future tokens as well. So we define plan prediction as predicting the context that spans from from $x_1$ to $x_{t+n}$, where $n$ is a hyperparameter. Now, we need to generate ground truth for these future contexts. Here, we must observe that all the transformations that occur in our encoder layers amount to an aggregation of the model input embeddings in a different latent space. Hence, we can expect an affinity between encoder output and a more direct agggregation of the model input embeddings. Thus, future context can be constructed from the aggregation of model input embeddings.

Finally, once the future contexts are constructed, then the objective function is defined as the following minimization. 


## Results

> All training runs below were done on a wikipedia dataset for 9k steps on a single A100 GPU, unless otherwise stated.
> 
> Implementation of decoder-only transformer model (baseline) can be found in the `baseline_transformer` directory in this repo


## Next steps

These are some further things to look forward to:
TODO

## Conclusions

---
## Appendix
### Run configs
TODO