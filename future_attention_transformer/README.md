# Future Attention Transformer [WIP readme]
> NB: LaTeX here is optimized for Github's Markdown, so please view it on Github.

Decoder-only transformer models apply a causal mask to enable parallel training with teacher forcing. But that wastes half of the attention matrix. What if you could use the upper-right triangle while respecting the temporal causality? The model presented here takes advantage of this observation.

## Motivations

TODO

## Architecture

TODO

## Analysis/experiments

TODO

## Conclusions

TODO