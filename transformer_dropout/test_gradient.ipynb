{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LearnedDropoutFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, A, B):\n",
    "        ctx.save_for_backward(x, A, B)\n",
    "        dropout_mask = 0.5 * torch.cos(A * x + B) + 0.5\n",
    "        return x * dropout_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, A, B = ctx.saved_tensors\n",
    "        dropout_mask = 0.5 * torch.cos(A * x + B) + 0.5\n",
    "        # Compute the gradient of the output with respect to the input (x)\n",
    "        grad_x = grad_output * dropout_mask\n",
    "        # Here you can modify grad_x according to your specific needs\n",
    "        # For example, apply your scaling method here\n",
    "        grad_x_scaled = grad_x * 0.5  # Example scaling, replace with your method\n",
    "        \n",
    "        # Compute gradients for A and B as None since we don't need to modify them here\n",
    "        grad_A = grad_B = None\n",
    "        return grad_x_scaled, grad_A, grad_B\n",
    "\n",
    "class LearnedDropout(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super(LearnedDropout, self).__init__()\n",
    "        self.A = nn.Parameter(torch.randn(dim_in))\n",
    "        self.B = nn.Parameter(torch.randn(dim_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LearnedDropoutFunction.apply(x, self.A, self.B)\n",
    "    \n",
    "\n",
    "x = LearnedDropout(10)(torch.randn(10))\n",
    "x.backward()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
