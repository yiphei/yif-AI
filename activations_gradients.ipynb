{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = list(set(''.join(words)))\n",
    "chars.sort()\n",
    "ctoi = {c: i + 1 for i, c in enumerate(chars)}\n",
    "ctoi[\".\"] = 0\n",
    "itoc = {i: c for c, i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "context_size = 3\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for word in words:\n",
    "    context = [0] * context_size\n",
    "    for char in word:\n",
    "        xs.append(context)\n",
    "        ys.append(ctoi[char])\n",
    "        context = context[1:] + [ctoi[char]]\n",
    "    \n",
    "    xs.append(context)\n",
    "    ys.append(0)\n",
    "\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(xs.shape[0] * 0.8)\n",
    "dev_size = int(xs.shape[0] * 0.1)\n",
    "\n",
    "train_xs = xs[:train_size]\n",
    "train_ys = ys[:train_size]\n",
    "\n",
    "dev_xs = xs[train_size:train_size + dev_size]\n",
    "dev_ys = ys[train_size:train_size + dev_size]\n",
    "\n",
    "test_xs = xs[train_size + dev_size:]\n",
    "test_ys = ys[train_size + dev_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 10))\n",
    "W1 = torch.randn((30, 200)) *5/3 / (30) ** 0.5\n",
    "B1 = torch.randn((200))\n",
    "W2 = torch.randn((200, 27)) / (200) ** 0.5\n",
    "B2 = torch.randn((27))\n",
    "params = [C, W1, B1, W2, B2]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(0, 1 ,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for i in range(200000):\n",
    "    #minibatch\n",
    "    mini_batch = torch.randint(0, train_xs.shape[0 ], (300,))\n",
    "\n",
    "    embed = C[train_xs[mini_batch]]\n",
    "    firt_layer_logits = torch.tanh(embed.view(-1,30) @ W1 + B1)\n",
    "    second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "    loss = F.cross_entropy(second_layer_logits, train_ys[mini_batch])\n",
    "\n",
    "    # backward\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data += -0.01 * p.grad\n",
    "\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3637826442718506\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embed = C[test_xs]\n",
    "    firt_layer_logits = torch.tanh(embed.view(-1,30) @ W1 + B1)\n",
    "    second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "    loss = F.cross_entropy(second_layer_logits, test_ys)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zayla.\n",
      "tavik.\n",
      "kawardia.\n",
      "skylean.\n",
      "raelia.\n",
      "casea.\n",
      "alinnleighby.\n",
      "saijamsia.\n",
      "serankaiyah.\n",
      "jandi.\n",
      "bevah.\n",
      "shrie.\n",
      "vaire.\n",
      "kadali.\n",
      "bernnley.\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    word = []\n",
    "    idxs = [0] * context_size\n",
    "    while True:\n",
    "        embed = C[torch.tensor([idxs])]\n",
    "        firt_layer_logits = torch.tanh(embed.view(-1,30) @ W1 + B1)\n",
    "        second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "        probs = F.softmax(second_layer_logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        word.append(itoc[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "        \n",
    "        idxs = idxs[1:] + [ix]\n",
    "    \n",
    "    print(\"\".join(word))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
