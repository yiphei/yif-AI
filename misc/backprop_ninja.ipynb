{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  4],\n",
       "        [18, 14,  1],\n",
       "        [11,  5,  9],\n",
       "        [ 0,  0,  1],\n",
       "        [12, 15, 14],\n",
       "        [ 0, 17,  1],\n",
       "        [ 0,  0, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 8, 25, 12],\n",
       "        [ 0,  0, 26],\n",
       "        [22, 15, 14],\n",
       "        [19, 13,  9],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  4,  5],\n",
       "        [ 5, 14,  9],\n",
       "        [18,  5,  5],\n",
       "        [ 0,  4,  1],\n",
       "        [ 1, 18,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  5, 12],\n",
       "        [ 0, 10,  1],\n",
       "        [ 9, 14,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 18],\n",
       "        [20,  5,  1],\n",
       "        [ 0, 11, 15],\n",
       "        [ 0,  0,  7],\n",
       "        [ 0, 18,  5],\n",
       "        [26,  5, 18],\n",
       "        [ 0,  0, 14],\n",
       "        [ 3,  5, 14],\n",
       "        [ 0, 18, 15]])"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3240, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = - 1.0 / n\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "dprobs = 1 / probs * dlogprobs\n",
    "\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim= 1 , keepdim=True)\n",
    "\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "dcounts_sum = -(counts_sum ** -2) * dcounts_sum_inv\n",
    "\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "dcounts = dcounts_sum * torch.ones_like(counts) + counts_sum_inv * dprobs\n",
    "\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "dlogit_maxes = -1 * dnorm_logits.sum(dim=1, keepdim=True)\n",
    "\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "dlogits = dnorm_logits + F.one_hot(logits.max(1).indices,num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "cmp('h', dh, h)\n",
    "\n",
    "dW2 = h.T @ dlogits                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "\n",
    "cmp('W2', dW2, W2)\n",
    "\n",
    "db2 = dlogits.sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "dhpreact = (1 - h**2) * dh\n",
    "\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "dbngain = (bnraw * dhpreact).sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "dbnbias = dhpreact.sum(dim = 0, keepdim=True)\n",
    "\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "\n",
    "\n",
    "dbnraw = bngain * dhpreact\n",
    "\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "dbnvar = -((1/2)*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "dbndiff2 = (dbnvar * (1/(n-1))) * (torch.ones_like(bndiff2))\n",
    "\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "\n",
    "dbndiff = bndiff * 2 * dbndiff2 + bnvar_inv * dbnraw\n",
    "\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "dbnmeani = torch.ones_like(bnmeani) * -1 * dbndiff.sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "dhprebn = dbndiff + torch.ones_like(hprebn) * (1/n) * dbnmeani\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "cmp('embcat', dembcat, embcat)\n",
    "\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "cmp('W1', dW1, W1)\n",
    "\n",
    "\n",
    "db1 = dhprebn.sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "\n",
    "\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(len(demb)):\n",
    "    for j in range(len(demb[i])):\n",
    "        dC[Xb[i][j]] += demb[i][j]\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3239712715148926 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "\n",
    "# dlogits = dlogprobs * (1 - (counts) * counts_sum ** -1)\n",
    "\n",
    "# dlogprobs = torch.zeros_like(logprobs)\n",
    "# dlogprobs[range(n), Yb] = 1.0 / n\n",
    "with torch.no_grad():\n",
    "    dlogits = -probs.clone()\n",
    "    dlogits[range(n), Yb] += 1.0\n",
    "    dlogits /= -n\n",
    "# dlogits *= dlogprobs\n",
    "# dlogits = None # TODO. my solution is 3 lines\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8011e+13,  1.0257e+14,  2.1672e+13,  4.9774e+13,  2.2396e+13,\n",
       "          8.7558e+13,  2.5106e+13,  3.6542e+13, -1.1410e+15,  3.2615e+13,\n",
       "          4.8670e+13,  3.9535e+13,  4.4379e+13,  3.5491e+13,  4.7233e+13,\n",
       "          1.5237e+13,  9.4617e+12,  2.1976e+13,  1.7742e+13,  5.7780e+13,\n",
       "          6.0250e+13,  2.4043e+13,  3.1469e+13,  8.0125e+13,  5.8327e+13,\n",
       "          3.0706e+13,  2.6033e+13],\n",
       "        [ 5.8675e+13,  6.4133e+13,  9.0721e+13,  5.6498e+13,  3.8036e+13,\n",
       "          3.9074e+13,  2.0455e+13,  5.0455e+13,  2.3940e+13,  2.5559e+13,\n",
       "          6.5839e+13,  4.6257e+13,  5.4363e+13,  3.0639e+13, -1.0994e+15,\n",
       "          3.9603e+13,  2.8479e+13,  2.0645e+13,  2.6527e+13,  4.7615e+13,\n",
       "          2.4470e+13,  2.8719e+13,  1.6519e+13,  7.0016e+13,  2.4536e+13,\n",
       "          4.0262e+13,  3.1033e+13],\n",
       "        [ 2.1623e+13,  2.6155e+13,  1.5796e+13,  1.4161e+13,  2.6799e+13,\n",
       "          4.4799e+13,  6.7570e+13,  6.3873e+13,  6.8781e+13,  3.2541e+13,\n",
       "          2.7334e+13,  3.7226e+13,  5.6647e+13,  5.8428e+13,  3.1518e+13,\n",
       "         -1.1340e+15,  1.5448e+13,  3.8249e+13,  3.1760e+13,  1.1702e+14,\n",
       "          7.8862e+13,  3.9475e+13,  4.9552e+13,  4.1024e+13,  3.8042e+13,\n",
       "          2.2684e+13,  3.2340e+13],\n",
       "        [ 3.4587e+13,  2.9775e+13,  4.8187e+13,  5.5280e+13,  6.2949e+13,\n",
       "          3.2811e+13,  4.6324e+13,  4.0617e+13,  5.8395e+13,  1.8687e+13,\n",
       "          4.4795e+13,  2.7542e+13,  4.1580e+13,  4.7825e+13,  7.7189e+13,\n",
       "          6.4502e+13,  2.6735e+13,  3.0635e+13,  1.9735e+13,  6.8372e+13,\n",
       "          2.1970e+13,  2.8843e+13, -1.1172e+15,  2.2389e+13,  2.6780e+13,\n",
       "          5.5671e+13,  4.8718e+13],\n",
       "        [-1.1441e+15,  1.9319e+13,  2.8304e+13,  2.5679e+13,  2.0494e+13,\n",
       "          4.4180e+13,  3.8395e+13,  1.7674e+13,  6.4631e+13,  3.9417e+13,\n",
       "          4.1543e+13,  1.6566e+13,  4.5274e+13,  2.8563e+13,  6.1361e+13,\n",
       "          1.7683e+14,  2.1087e+13,  6.1552e+13,  5.7053e+13,  7.1821e+13,\n",
       "          4.9632e+13,  1.8623e+13,  2.3595e+13,  1.7268e+13,  4.6256e+13,\n",
       "          2.6169e+13,  4.6481e+13],\n",
       "        [ 4.0368e+13,  4.0241e+13,  6.0021e+13,  6.9900e+13,  6.7723e+13,\n",
       "          3.1123e+13,  3.3126e+13,  5.4653e+13,  3.2185e+13,  1.7049e+13,\n",
       "          5.2299e+13,  2.0564e+13,  3.3845e+13,  4.6669e+13,  7.8857e+13,\n",
       "          4.9504e+13,  1.8496e+13,  2.6341e+13,  1.2206e+13, -1.1228e+15,\n",
       "          3.8843e+13,  3.4724e+13,  4.8319e+13,  1.8687e+13,  1.8659e+13,\n",
       "          9.3921e+13,  4.8148e+13],\n",
       "        [ 5.9732e+13,  1.3731e+13,  7.7323e+13,  1.6767e+13,  3.2749e+13,\n",
       "          2.4538e+13,  6.8925e+13,  5.3422e+13,  5.8671e+13, -1.1182e+15,\n",
       "          2.3276e+13,  3.8563e+13,  3.2625e+13,  2.2970e+13,  2.1672e+13,\n",
       "          6.7264e+13,  8.7218e+13,  5.3987e+13,  1.2180e+13,  4.1964e+13,\n",
       "          3.4073e+13,  5.9438e+13,  5.3711e+13,  2.3843e+13,  3.9399e+13,\n",
       "          3.0259e+13,  3.3619e+13],\n",
       "        [ 4.8711e+13,  3.7302e+13,  2.8030e+13,  1.2010e+14,  2.9331e+13,\n",
       "          5.6671e+13,  3.3910e+13,  8.5436e+13,  4.2576e+13,  9.8244e+12,\n",
       "          3.6730e+13,  1.5998e+13,  2.4534e+13,  6.4649e+13, -1.1385e+15,\n",
       "          1.6433e+13,  1.4756e+13,  1.6142e+13,  1.7664e+13,  1.0626e+14,\n",
       "          3.7886e+13,  5.6164e+13,  3.2184e+13,  4.0898e+13,  2.4767e+13,\n",
       "          6.4007e+13,  4.1223e+13],\n",
       "        [ 5.7905e+13,  3.5943e+13,  1.9568e+13,  3.9235e+13,  1.2205e+13,\n",
       "         -1.1171e+15,  5.5516e+13,  8.9269e+13,  3.1027e+13,  2.3623e+13,\n",
       "          3.9146e+13,  3.5199e+13,  4.6137e+13,  2.6817e+13,  1.5281e+13,\n",
       "          2.0648e+13,  2.0901e+13,  4.6957e+13,  2.6005e+13,  9.6720e+13,\n",
       "          4.5991e+13,  9.6710e+13,  4.8991e+13,  4.8102e+13,  4.4584e+13,\n",
       "          1.9366e+13,  3.8904e+13],\n",
       "        [ 3.0689e+13, -1.1455e+15,  3.4444e+13,  2.1074e+13,  3.4784e+13,\n",
       "          4.1439e+13,  3.7333e+13,  3.0145e+13,  9.6577e+13,  6.4156e+13,\n",
       "          1.7744e+13,  4.1567e+13,  4.8885e+13,  5.1550e+13,  3.3886e+13,\n",
       "          2.9838e+13,  6.5109e+13,  4.9705e+13,  4.6342e+13,  2.9606e+13,\n",
       "          7.7874e+13,  3.0873e+13,  7.5641e+13,  2.1684e+13,  3.2312e+13,\n",
       "          4.3857e+13,  2.2057e+13],\n",
       "        [ 1.7465e+13,  8.7678e+12,  3.9503e+13,  4.5177e+13,  3.9062e+13,\n",
       "          4.0813e+13,  2.1221e+13,  7.1762e+12,  4.2721e+13,  5.1358e+13,\n",
       "          3.0739e+13,  2.0414e+13,  6.2356e+13,  3.4400e+13,  5.2620e+13,\n",
       "          1.5433e+14,  5.5305e+13,  6.6362e+13,  9.1116e+13,  4.1283e+13,\n",
       "         -1.1087e+15,  1.1562e+13,  1.3813e+13,  2.4827e+13,  4.0982e+13,\n",
       "          3.2399e+13,  2.6626e+13],\n",
       "        [ 2.5861e+13,  3.5892e+13,  2.5685e+13, -9.4787e+14,  1.3464e+13,\n",
       "          6.2864e+13,  7.2173e+13,  4.8194e+13,  5.0976e+13,  3.2154e+13,\n",
       "          1.8585e+13,  1.9182e+13,  2.3743e+13,  3.2920e+13,  8.7094e+12,\n",
       "          7.3585e+12,  2.3094e+13,  4.1226e+13,  1.8811e+13,  6.3657e+13,\n",
       "          6.0789e+13,  4.3775e+13,  2.3164e+13,  3.9465e+13,  4.9515e+13,\n",
       "          5.2545e+13,  1.7746e+13],\n",
       "        [ 2.0152e+13,  1.6162e+13,  5.6772e+13,  1.5968e+13,  2.8400e+13,\n",
       "          1.7921e+13,  9.4466e+13,  3.5832e+13, -1.0972e+15,  5.3890e+13,\n",
       "          2.1634e+13,  4.1327e+13,  4.6918e+13,  3.3321e+13,  2.0005e+13,\n",
       "          4.3050e+13,  8.4505e+13,  5.2176e+13,  4.6511e+13,  2.7505e+13,\n",
       "          5.5974e+13,  4.5480e+13,  1.0774e+14,  1.2396e+13,  2.2199e+13,\n",
       "          2.8557e+13,  3.2010e+13],\n",
       "        [ 2.4071e+13,  3.1948e+13,  6.0051e+13,  2.1743e+13,  1.9334e+13,\n",
       "          7.5755e+13,  3.1838e+13,  1.7252e+13,  3.3267e+13,  4.8957e+13,\n",
       "          3.6909e+13,  3.4992e+13,  7.5788e+13,  3.9678e+13, -1.0951e+15,\n",
       "          4.8825e+13,  3.1378e+13,  3.9250e+13,  9.5467e+13,  1.7384e+13,\n",
       "          7.4429e+13,  1.8616e+13,  1.9172e+13,  4.0825e+13,  3.6862e+13,\n",
       "          4.1248e+13,  4.3725e+13],\n",
       "        [ 5.3261e+13,  3.1447e+13,  3.2078e+13,  5.3718e+13,  1.7950e+13,\n",
       "          3.6178e+13,  3.4005e+13,  5.9946e+13,  6.0558e+13,  5.2967e+13,\n",
       "          3.8766e+13,  5.1924e+13, -1.1008e+15,  4.2924e+13,  3.6370e+13,\n",
       "          2.5147e+13,  2.2540e+13,  6.2422e+13,  3.3101e+13,  5.4186e+13,\n",
       "          6.3644e+13,  5.9723e+13,  3.2270e+13,  3.6535e+13,  2.2173e+13,\n",
       "          2.4162e+13,  2.6509e+13],\n",
       "        [-1.1168e+15,  6.5762e+13,  3.4133e+13,  1.9426e+13,  2.2376e+13,\n",
       "          9.8596e+13,  3.3016e+13,  2.4526e+13,  3.0409e+13,  5.5121e+13,\n",
       "          4.2526e+13,  4.8872e+13,  7.0894e+13,  1.9548e+13,  3.9668e+13,\n",
       "          1.6159e+13,  4.2455e+13,  2.2558e+13,  6.7257e+13,  2.1844e+13,\n",
       "          4.7606e+13,  3.1012e+13,  3.8962e+13,  7.1799e+13,  5.6379e+13,\n",
       "          3.4690e+13,  2.4926e+13],\n",
       "        [ 3.1240e+13,  2.8821e+13,  8.8072e+13,  4.8412e+13,  6.4259e+13,\n",
       "          6.0996e+13,  1.7945e+13,  2.7488e+13,  2.3478e+13,  1.9555e+13,\n",
       "          5.9999e+13, -1.1401e+15,  4.6125e+13,  5.9146e+13,  8.9291e+13,\n",
       "          5.9510e+13,  1.6534e+13,  2.1206e+13,  2.9379e+13,  6.0474e+13,\n",
       "          3.3463e+13,  1.9790e+13,  1.1010e+13,  5.1867e+13,  2.4421e+13,\n",
       "          5.1649e+13,  5.9607e+13],\n",
       "        [-1.1103e+15,  9.1940e+13,  5.1095e+13,  5.7757e+13,  4.3740e+13,\n",
       "          3.9893e+13,  2.3101e+13,  7.4000e+13,  2.8398e+13,  1.7589e+13,\n",
       "          9.0304e+13,  1.3599e+13,  2.7979e+13,  2.1313e+13,  5.4165e+13,\n",
       "          2.7866e+13,  1.5711e+13,  3.4806e+13,  1.9858e+13,  6.4760e+13,\n",
       "          2.5546e+13,  5.2985e+13,  3.0212e+13,  2.2963e+13,  2.4776e+13,\n",
       "          4.9397e+13,  7.0257e+13],\n",
       "        [ 2.0152e+13,  1.6162e+13,  5.6772e+13,  1.5968e+13,  2.8400e+13,\n",
       "          1.7921e+13,  9.4466e+13,  3.5832e+13,  6.5023e+13,  5.3890e+13,\n",
       "          2.1634e+13,  4.1327e+13,  4.6918e+13,  3.3321e+13,  2.0005e+13,\n",
       "          4.3050e+13,  8.4505e+13,  5.2176e+13,  4.6511e+13,  2.7505e+13,\n",
       "          5.5974e+13,  4.5480e+13,  1.0774e+14,  1.2396e+13,  2.2199e+13,\n",
       "          2.8557e+13, -1.1302e+15],\n",
       "        [ 2.5108e+13,  2.5451e+13,  3.5976e+13,  1.6584e+13,  4.2479e+13,\n",
       "          3.7202e+13,  7.9364e+13,  4.9193e+13,  4.0225e+13, -1.1246e+15,\n",
       "          4.0947e+13,  4.7398e+13,  7.2660e+13,  5.9656e+13,  2.0972e+13,\n",
       "          2.5802e+13,  3.1699e+13,  2.3852e+13,  3.5881e+13,  9.2772e+13,\n",
       "          5.9007e+13,  3.7079e+13,  4.7105e+13,  5.4146e+13,  3.3329e+13,\n",
       "          2.4747e+13,  2.9684e+13],\n",
       "        [ 5.7760e+13,  1.6429e+13,  1.8467e+13,  4.2571e+13,  3.2654e+13,\n",
       "          4.2941e+13,  3.2271e+13,  3.9672e+13,  6.0551e+13,  2.1988e+13,\n",
       "          7.8979e+13,  3.8545e+13,  6.8517e+13,  2.6395e+13,  5.9250e+13,\n",
       "          7.1516e+13,  1.8106e+13,  2.4897e+13,  4.4184e+13,  6.6844e+13,\n",
       "          2.4454e+13,  3.5711e+13,  2.7157e+13,  2.8592e+13,  3.6425e+13,\n",
       "         -1.0957e+15,  4.4542e+13],\n",
       "        [-1.1068e+15,  7.7502e+13,  5.5212e+13,  4.8585e+13,  2.8993e+13,\n",
       "          7.1295e+13,  3.0258e+13,  4.6167e+13,  2.7487e+13,  1.8408e+13,\n",
       "          6.5058e+13,  2.6792e+13,  2.7319e+13,  3.4406e+13,  5.6064e+13,\n",
       "          4.5930e+13,  2.1215e+13,  4.3492e+13,  2.4218e+13,  1.0932e+14,\n",
       "          1.6220e+13,  3.7059e+13,  1.5988e+13,  3.3868e+13,  3.4980e+13,\n",
       "          3.3314e+13,  4.1346e+13],\n",
       "        [ 2.0152e+13, -1.1461e+15,  5.6772e+13,  1.5968e+13,  2.8400e+13,\n",
       "          1.7921e+13,  9.4466e+13,  3.5832e+13,  6.5023e+13,  5.3890e+13,\n",
       "          2.1634e+13,  4.1327e+13,  4.6918e+13,  3.3321e+13,  2.0005e+13,\n",
       "          4.3050e+13,  8.4505e+13,  5.2176e+13,  4.6511e+13,  2.7505e+13,\n",
       "          5.5974e+13,  4.5480e+13,  1.0774e+14,  1.2396e+13,  2.2199e+13,\n",
       "          2.8557e+13,  3.2010e+13],\n",
       "        [ 4.8749e+13, -1.1409e+15,  2.9889e+13,  1.3841e+13,  5.9570e+13,\n",
       "          2.9420e+13,  6.8519e+13,  3.0756e+13,  3.0651e+13,  4.3233e+13,\n",
       "          3.9019e+13,  4.0881e+13,  4.2252e+13,  4.0464e+13,  4.9890e+13,\n",
       "          4.6622e+13,  8.2907e+13,  2.6828e+13,  4.3672e+13,  2.0522e+13,\n",
       "          2.3202e+13,  2.6775e+13,  1.3257e+14,  2.0814e+13,  5.5553e+13,\n",
       "          3.8702e+13,  1.9328e+13],\n",
       "        [ 3.6934e+13,  9.4746e+13,  2.4317e+13,  5.0734e+13,  3.0656e+13,\n",
       "          7.1908e+13,  2.5654e+13, -1.1309e+15,  2.9348e+13,  2.8605e+13,\n",
       "          1.1762e+14,  1.8388e+13,  3.5526e+13,  1.2714e+13,  4.9447e+13,\n",
       "          2.0127e+13,  1.7492e+13,  1.8655e+13,  1.5491e+13,  3.4298e+13,\n",
       "          1.5737e+13,  2.8058e+13,  1.2203e+13,  1.7102e+14,  5.2484e+13,\n",
       "          2.7627e+13,  5.4806e+13],\n",
       "        [ 6.9315e+13,  2.9604e+13,  1.5883e+13,  2.3193e+13,  3.2415e+13,\n",
       "          9.3367e+13,  4.5593e+13,  3.4182e+13,  3.0860e+13,  3.5803e+13,\n",
       "          3.9033e+13,  5.5525e+13,  6.6414e+13,  4.0990e+13,  2.9387e+13,\n",
       "          1.3702e+13,  3.5371e+13,  2.4227e+13, -1.0971e+15,  1.7676e+13,\n",
       "          3.0829e+13,  2.5496e+13,  7.8485e+13,  5.6047e+13,  7.4153e+13,\n",
       "          5.0120e+13,  1.3062e+13],\n",
       "        [ 4.1734e+13,  2.7601e+13,  2.3852e+13,  1.1649e+13,  4.8446e+13,\n",
       "          2.3763e+13,  9.0665e+13,  3.8917e+13,  3.4740e+13, -1.1062e+15,\n",
       "          2.3156e+13,  4.5620e+13,  7.2826e+13,  3.7226e+13,  3.7224e+13,\n",
       "          1.9240e+13,  6.4259e+13,  3.1968e+13,  3.6487e+13,  4.1325e+13,\n",
       "          2.7731e+13,  4.0615e+13,  1.4630e+14,  2.1419e+13,  3.9072e+13,\n",
       "          3.1611e+13,  1.2464e+13],\n",
       "        [ 2.8662e+13,  3.3614e+13,  5.6484e+13, -1.1394e+15,  3.9759e+13,\n",
       "          2.4614e+13,  4.3226e+13,  3.1536e+13,  4.3021e+13,  5.6522e+13,\n",
       "          2.5695e+13,  3.2652e+13,  4.1751e+13,  4.0457e+13,  5.8277e+13,\n",
       "          6.3160e+13,  4.8790e+13,  5.4219e+13,  5.5642e+13,  1.8925e+13,\n",
       "          1.1535e+14,  1.8684e+13,  5.0906e+13,  1.4148e+13,  1.7599e+13,\n",
       "          3.9112e+13,  5.0304e+13],\n",
       "        [ 7.2827e+13,  4.5057e+13,  1.6996e+13,  2.1561e+13,  3.7911e+13,\n",
       "         -1.0926e+15,  4.5730e+13,  3.9863e+13,  1.1301e+13,  2.9384e+13,\n",
       "          5.6423e+13,  3.1396e+13,  3.0343e+13,  2.3363e+13,  3.6685e+13,\n",
       "          3.2624e+13,  7.8716e+13,  2.9280e+13,  7.3978e+13,  2.8787e+13,\n",
       "          3.9873e+13,  3.7274e+13,  5.1818e+13,  6.5342e+13,  8.2402e+13,\n",
       "          1.4574e+13,  2.2748e+13],\n",
       "        [ 2.3904e+13,  1.9200e+13,  3.6672e+13,  3.5406e+13,  5.4182e+13,\n",
       "          2.5271e+13,  7.1581e+13,  3.2306e+13,  8.4254e+13, -1.1309e+15,\n",
       "          3.6728e+13,  3.7867e+13,  3.7366e+13,  4.3682e+13,  4.5272e+13,\n",
       "          6.4547e+13,  3.4291e+13,  3.2067e+13,  4.2203e+13,  3.6703e+13,\n",
       "          4.9525e+13,  3.3387e+13,  6.6997e+13,  1.6003e+13,  3.2542e+13,\n",
       "          6.1338e+13,  4.1294e+13],\n",
       "        [-1.1190e+15,  5.5181e+13,  2.6573e+13,  7.3020e+13,  5.1851e+13,\n",
       "          6.0879e+13,  3.5070e+13,  3.1404e+13,  2.7920e+13,  2.1043e+13,\n",
       "          4.6309e+13,  2.3339e+13,  2.1598e+13,  3.0463e+13,  1.6339e+13,\n",
       "          3.4663e+13,  3.7372e+13,  3.1778e+13,  5.0819e+13,  5.5699e+13,\n",
       "          6.9980e+13,  3.2957e+13,  2.3959e+13,  8.4117e+13,  8.5783e+13,\n",
       "          2.4189e+13,  3.0412e+13],\n",
       "        [ 5.2876e+13,  2.6418e+13,  7.0040e+13,  1.9073e+13,  8.0427e+13,\n",
       "          3.1421e+13,  1.7790e+13,  2.3588e+13,  1.8790e+13,  6.1208e+13,\n",
       "          3.2763e+13,  3.4194e+13,  3.8883e+13,  8.5180e+13,  5.4000e+13,\n",
       "          4.0401e+13,  5.4384e+13,  5.4077e+13, -1.1124e+15,  1.6736e+13,\n",
       "          6.9440e+13,  1.2635e+13,  6.7692e+13,  2.5125e+13,  2.1699e+13,\n",
       "          4.5113e+13,  2.2110e+13]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64]) torch.Size([1, 64]) torch.Size([32, 64])\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "\n",
    "dhprebn = bngain * bnvar_inv * n ** -1 * (n * dhpreact - dhpreact.sum(dim=0, keepdim=True) - (n) * (n - 1) ** -1 * bnraw * (dhpreact * bnraw).sum(dim=0, keepdim=True))\n",
    "\n",
    "print(hprebn.shape, bnmeani.shape, hpreact.shape)\n",
    "\n",
    "# dhprebn = None # TODO. my solution is 1 (long) line\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
