{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../datasets/names/names.txt', 'r').read().splitlines()\n",
    "chars = list(set(''.join(words)))\n",
    "chars.sort()\n",
    "ctoi = {c: i + 1 for i, c in enumerate(chars)}\n",
    "ctoi[\".\"] = 0\n",
    "itoc = {i: c for c, i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "context_size = 3\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for word in words:\n",
    "    context = [0] * context_size\n",
    "    for char in word:\n",
    "        xs.append(context)\n",
    "        ys.append(ctoi[char])\n",
    "        context = context[1:] + [ctoi[char]]\n",
    "    \n",
    "    xs.append(context)\n",
    "    ys.append(0)\n",
    "\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(xs.shape[0] * 0.8)\n",
    "dev_size = int(xs.shape[0] * 0.1)\n",
    "\n",
    "train_xs = xs[:train_size]\n",
    "train_ys = ys[:train_size]\n",
    "\n",
    "dev_xs = xs[train_size:train_size + dev_size]\n",
    "dev_ys = ys[train_size:train_size + dev_size]\n",
    "\n",
    "test_xs = xs[train_size + dev_size:]\n",
    "test_ys = ys[train_size + dev_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "embedding_context = embedding_size * context_size\n",
    "hidden_layer_size = 200\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((len(ctoi.keys()), embedding_size), generator=g)\n",
    "W1 = torch.randn((embedding_context, hidden_layer_size), generator=g) *5/3 / (embedding_context) ** 0.5\n",
    "B1 = torch.randn((hidden_layer_size), generator=g)\n",
    "GAMMA1 = torch.ones((1, hidden_layer_size))\n",
    "BETA1 = torch.zeros((1, hidden_layer_size))\n",
    "\n",
    "W2 = torch.randn((hidden_layer_size, len(ctoi.keys())), generator=g) / (hidden_layer_size) ** 0.5\n",
    "B2 = torch.randn(len(ctoi.keys()), generator=g)\n",
    "params = [C, W1, B1, GAMMA1, BETA1, W2, B2]\n",
    "\n",
    "running_mean = torch.zeros((1, hidden_layer_size))\n",
    "running_std = torch.ones((1, hidden_layer_size))\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.323450803756714\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 300\n",
    "\n",
    "for i in range(2000):\n",
    "    #minibatch\n",
    "    mini_batch = torch.randint(0, train_xs.shape[0], (batch_size,), generator=g)\n",
    "\n",
    "    embed = C[train_xs[mini_batch]]\n",
    "    first_layer = embed.view(-1,embedding_context) @ W1 + B1\n",
    "    \n",
    "    first_layer_mean = first_layer.mean(0, keepdim = True)\n",
    "    first_layer_std = first_layer.std(0, keepdim = True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_mean = 0.999 * running_mean + 0.001 * first_layer_mean\n",
    "        running_std = 0.999 * running_std + 0.001 * first_layer_std\n",
    "\n",
    "    normalized_first_layer =  GAMMA1 * ((first_layer - first_layer_mean)/ first_layer_std) + BETA1\n",
    "    firt_layer_logits = torch.tanh(normalized_first_layer)\n",
    "\n",
    "    second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "    loss = F.cross_entropy(second_layer_logits, train_ys[mini_batch])\n",
    "\n",
    "    # backward\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i < 100000 else 0.01 \n",
    "    for p in params:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3246591091156006\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embed = C[dev_xs]\n",
    "    first_layer = embed.view(-1,embedding_context) @ W1 + B1\n",
    "    normalized_first_layer =  GAMMA1 * ((first_layer - running_mean)/ running_std) + BETA1\n",
    "    firt_layer_logits = torch.tanh(normalized_first_layer)\n",
    "\n",
    "    second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "    loss = F.cross_entropy(second_layer_logits, dev_ys)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3246593475341797\n"
     ]
    }
   ],
   "source": [
    "W_new = GAMMA1 * W1 / running_std\n",
    "B_new = GAMMA1 * (B1 - running_mean) / running_std + BETA1\n",
    "with torch.no_grad():\n",
    "    embed = C[dev_xs]\n",
    "    first_layer = embed.view(-1,embedding_context) @ W_new + B_new\n",
    "    firt_layer_logits = torch.tanh(first_layer)\n",
    "\n",
    "    second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "    loss = F.cross_entropy(second_layer_logits, dev_ys)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhanalahahdabgbaqhrhagharahcwandch.\n",
      "ddoyadhanahbhbhshbulgzauhmeldqynthadhynzchvnobgdacshmahhandrxv.\n",
      "hhanzhth.\n",
      "avddhngrwh.\n",
      "rhmlllahlynghnryllhbrbaathghhaddhohahhhbhhah.\n",
      "xhdohnah.\n",
      "amdabhnbbrnqhahahyannighhahnlivhhqhmbdilhrdni.\n",
      "jhhadd.\n",
      "khvbuv.\n",
      "bldhhd.\n",
      "jamhahldamalqhnnahldzahxlluduhhhhdqthmdizgbnedhhhhalddhdhhndhbnnbbhchrennzlohghblohabvthvghhahlan.\n",
      "dddynhhhhbahghdhbas.\n",
      "vkhlgvabnmuhhadhahhbandamar.\n",
      "dvahbuhhnqh.\n",
      "rhbbzxrldhmahdohvindhahdrithlah.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(15):\n",
    "        word = []\n",
    "        idxs = [0] * context_size\n",
    "        while True:\n",
    "            embed = C[torch.tensor([idxs])]\n",
    "            firt_layer_logits = torch.tanh(embed.view(-1,embedding_context) @ W1 + B1)\n",
    "            second_layer_logits = firt_layer_logits @ W2 + B2\n",
    "            probs = F.softmax(second_layer_logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            word.append(itoc[ix])\n",
    "            if ix == 0:\n",
    "                break\n",
    "            \n",
    "            idxs = idxs[1:] + [ix]\n",
    "        \n",
    "        print(\"\".join(word))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd-yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
