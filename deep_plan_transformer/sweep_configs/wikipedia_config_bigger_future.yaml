program: future_encoder_transformer/training_script.py
name: wikipedia_config_bigger_future
project: future_encoder_transformer
method: grid
metric:
  goal: minimize
  name: est_val_loss
parameters:
    model_config:
        parameters:
            use_bias:
                value: False
            context_size:
                value: 200
            n_embed: 
                value: 150
            n_layer: 
                value: 13
            n_head: 
                value: 5
            dropout_rate:
                value: 0
            future_context_size:
                values: [15, 30, 60]
            present_embed_normalization_type:
                values: [null,1]
            cross_attn_config:
                parameters:
                    n_head:
                        value: 10
                    use_bias:
                        value: False
            use_ln_on_encoder_out:
                value: True
            add_ln_before_decoder_ff:
                value: False
            encoder_loss_type:
                values: [2,3]
            encoder_loss_detach_type:
                values: [2,3]
            encoder_embed_ln_type:
                values: [2,3,4]
            encoder_loss_coeff:
                value: 1
            future_aggregation_type:
                values: [1,2]
    batch_size: 
        value: 50
    train_steps: 
        value: 9000
    lr: 
        value: 9e-4
    warmup_iters:
        value: 300
    min_lr: 
        value: 9e-5
    gradient_accumulation_steps: 
        value: 16
    lr_decay_iters: 
        value: 700000
    est_interval: 
        value: 500
    est_steps: 
        value: 200